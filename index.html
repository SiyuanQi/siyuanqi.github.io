<!doctype html>
<html class="no-js" lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="x-ua-compatible" content="ie=edge">
        <title>Siyuan Qi @ UCLA</title>
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">

        <link rel="shortcut icon" href="img/Q.jpg"/>
        <!-- Place favicon.ico in the root directory -->

        <link href="https://fonts.googleapis.com/css?family=Libre+Baskerville|Merriweather|Merriweather+Sans" rel="stylesheet">
        <link rel="stylesheet" href="css/animate.min.css">
        <link rel="stylesheet" href="css/normalize.css">
        <link rel="stylesheet" href="css/bootstrap.min.css">
        <link rel="stylesheet" href="css/bootflat.min.css">
        <link rel="stylesheet" href="css/font-awesome/css/font-awesome.css">
        <link rel="stylesheet" href="css/academicons/css/academicons.css">
        <link rel="stylesheet" href="css/main.css">
        <link rel="stylesheet" href="css/scrolling-nav.css">

        <script src="js/vendor/modernizr-2.8.3.min.js"></script>
    </head>
    <body class="animated fadeIn" id="page-top" data-spy="scroll" data-target=".navbar-fixed-top">
        <!--[if lte IE 9]>
            <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
        <![endif]-->

        <!-- Add your site or application content here -->
        <!-- Navigation -->
        <nav class="navbar navbar-default navbar-fixed-top" role="navigation" style="font-family: 'Merriweather Sans', sans-serif;">
            <div class="container">

                <div class="col-lg-8">
                    <div class="navbar-header page-scroll">
                        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
                            <span class="sr-only">Toggle navigation</span>
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                        </button>
                        <a class="navbar-brand page-scroll" href="#page-top">Siyuan Qi</a>
                    </div>

                    <!-- Collect the nav links, forms, and other content for toggling -->
                    <div class="collapse navbar-collapse navbar-ex1-collapse">
                        <ul class="nav navbar-nav">
                            <!-- Hidden li included to remove active class from about link when scrolled up past about section -->
                            <li class="hidden">
                                <a class="page-scroll" href="#page-top"></a>
                            </li>
                            <!--<li>-->
                                <!--<a class="page-scroll" href="#intro">Home</a>-->
                            <!--</li>-->
                            <li>
                                <a class="page-scroll" href="#about">About</a>
                            </li>
                            <li>
                                <a class="page-scroll" href="#news">News</a>
                            </li>
                            <li>
                                <a class="page-scroll" href="#projects">Projects</a>
                            </li>
                            <li>
                                <a class="page-scroll" href="#publications">Publications</a>
                            </li>
                            <li>
                                <a class="page-scroll" href="#more">More</a>
                            </li>
                        </ul>
                    </div>
                    <!-- /.navbar-collapse -->
                </div>


                <div class="col-lg-4">
                    <ul class="top-contact-info">
                        <li><a target="_self" href="mailto:syqi@cs.ucla.edu"><i class="fa fa-envelope"></i></a></li>
                        <li><a target="_blank" href="https://scholar.google.com/citations?hl=en&user=ePclJR4AAAAJ"><i class="ai ai-google-scholar"></i></a></li>
                        <li><a target="_blank" href="https://www.linkedin.com/in/siyuanqi"><i class="fa fa-linkedin"></i></a></li>
                        <li><a target="_blank" href="https://github.com/SiyuanQi"><i class="fa fa-github"></i></a></li>
                        <li><a target="_blank" href="https://soundcloud.com/siyuanqi"><i class="fa fa-soundcloud"></i></a></li>
                    </ul>
                </div>
            </div>
            <!-- /.container -->
        </nav>

        <!-- Intro Section -->
        <section id="intro" class="intro-section">
            <div class="intro-profile">
                <div class="intro-avatar">
                    <div class="section-title">
                        <img class="img-circle" src="img/avatar.jpg" alt="" height="150">
                    </div>
                </div>
                <div class="intro-info">
                    <div class="section-title">
                        <h3>Siyuan Qi</h3>
                        <h5>PhD Candidate @ UCLA CS</h5>
                    </div>
                    <!-- <a class="btn btn-default page-scroll" href="#about">Click Me to Scroll Down!</a> -->
                </div>
            </div>
        </section>

        <!-- About Section -->
        <section id="about" class="about-section">
            <div class="container">
                <div class="row">
                    <div class="col-lg-1"></div>
                    <div class="col-lg-10">
                        <div class="section-title"><h3>About Me</h3></div>
                        <p>I am a third year Ph.D. Candidate in the <a href="http://www.cs.ucla.edu/">Computer Science
                            Department</a> at <a href="http://www.ucla.edu/">University of California, Los Angeles</a>. I am
                            currently doing computer vision research in the <a href="http://vcla.stat.ucla.edu/"
                                                                               target="_blank">Center for Vision, Cognition,
                                Learning, and Autonomy</a> advised by <a href="http://www.stat.ucla.edu/~sczhu/"
                                                                         target="_blank">Professor Song-Chun Zhu</a>.</p>

                        <p>My research interests include Computer Vision, Machine Learning, and Cognitive Science.</p>
                        <blockquote>
                            <p>We who cut mere stones must always be envisioning cathedrals.</p>
                            <footer>Quarry worker's creed</footer>
                        </blockquote>
                    </div>
                    <div class="col-lg-1"></div>
                </div>
            </div>
        </section>

        <!-- News Section -->
        <section id="news" class="news-section">
            <div class="container">
                <div class="row">
                    <div class="col-lg-1"></div>
                    <div class="col-lg-10">
                        <div class="section-title"><h2>News</h2></div>
                    </div>
                    <div class="col-lg-1"></div>
                </div>

                <div class="row">
                    <div class="col-md-12">
                        <div class="timeline">
                            <dl>
                                <dt data-toggle="collapse" href="#news-2018" style="cursor: pointer">2018</dt>
                                <div id="news-2018" class="panel-collapse collapse in">
                                    <dd class="pos-right clearfix">
                                        <div class="circ"></div>
                                        <div class="time">2018 Feb</div>
                                        <div class="events">
                                            <div class="pull-left">
                                                <p class="events-object img-rounded"><i class="fa fa-file fa-lg"></i></p>
                                            </div>
                                            <div class="events-body">
                                                <h4 class="events-heading">One paper accepted at <a target="_blank" href="http://cvpr2018.thecvf.com/">CVPR 2018</a></h4>
                                                <p>Human-centric Indoor Scene Synthesis Using Stochastic Grammar.</p>
                                                <p>
                                                    [<a target="_blank" href="publications/cvpr2018synthesis/cvpr2018synthesis.pdf">paper</a>]
                                                    [<a target="_blank" href="publications/cvpr2018synthesis/cvpr2018synthesis_supplementary.pdf">supplementary</a>]
                                                    [<a target="_blank" href="https://github.com/SiyuanQi/human-centric-scene-synthesis">code</a>]]
                                                    [<a target="_blank" href="http://www.yzhu.io/projects/cvpr18_scenesynthesis/index.html">project</a>]
                                                </p>
                                                <p>
                                                    <span class="label label-primary">Conference</span>&nbsp;
                                                    <span class="label label-normal">Computer Vision</span>&nbsp;
                                                    <span class="label label-normal">Computer Graphics</span>&nbsp;
                                                </p>
                                            </div>
                                        </div>
                                    </dd>
                                    <dd class="pos-left clearfix">
                                        <div class="circ"></div>
                                        <div class="time">2018 Jan</div>
                                        <div class="events">
                                            <div class="pull-left">
                                                <p class="events-object img-rounded"><i class="fa fa-file fa-lg"></i></p>
                                            </div>
                                            <div class="events-body">
                                                <h4 class="events-heading">Two papers accepted at <a target="_blank" href="https://icra2018.org/">ICRA 2018</a></h4>
                                                <p>Intent-aware Multi-agent Reinforcement Learning.</p>
                                                <p>
                                                    [<a target="_blank" href="publications/icra2018intent/icra2018intent.pdf">paper</a>]
                                                    [<a target="_blank" href="https://youtu.be/UtYbynV9sK0">demo</a>]
                                                    [<a target="_blank" href="https://github.com/SiyuanQi/intentMARL">code</a>]
                                                </p>
                                                <p>Unsupervised Learning of Hierarchical Models for Hand-Object Interactions using Tactile Glove.</p>
                                                <p>
                                                    [<a target="_blank">paper coming soon</a>]
                                                </p>
                                                <p>
                                                    <span class="label label-primary">Conference</span>&nbsp;
                                                    <span class="label label-normal">Robotics</span>&nbsp;
                                                </p>
                                            </div>
                                        </div>
                                    </dd>
                                </div>
                                <dt data-toggle="collapse" href="#news-2017" style="cursor: pointer">2017</dt>
                                <div id="news-2017" class="panel-collapse collapse in">
                                    <dd class="pos-right clearfix">
                                        <div class="circ"></div>
                                        <div class="time">2017 Jul</div>
                                        <div class="events">
                                            <div class="pull-left">
                                                <p class="events-object img-rounded"><i class="fa fa-file fa-lg"></i></p>
                                            </div>
                                            <div class="events-body">
                                                <h4 class="events-heading">One paper accepted at <a target="_blank" href="http://iccv2017.thecvf.com/">ICCV 2017</a></h4>
                                                <p>Predicting Human Activities Using Stochastic Grammar.</p>
                                                <p>
                                                    [<a target="_blank" href="publications/iccv2017prediction/iccv2017prediction.pdf">paper</a>]
                                                    [<a target="_blank" href="https://youtu.be/z2gwj8AnA_c">demo</a>]
                                                    [<a target="_blank" href="https://github.com/SiyuanQi/grammar-activity-prediction">code</a>]
                                                </p>
                                                <p>
                                                    <span class="label label-primary">Conference</span>&nbsp;
                                                    <span class="label label-normal">Computer Vision</span>&nbsp;
                                                </p>
                                            </div>
                                        </div>
                                    </dd>
                                    <dd class="pos-left clearfix">
                                        <div class="circ"></div>
                                        <div class="time">2017 Jun</div>
                                        <div class="events">
                                            <div class="pull-left">
                                                <p class="events-object img-rounded"><i class="fa fa-file fa-lg"></i></p>
                                            </div>
                                            <div class="events-body">
                                                <h4 class="events-heading">One paper accepted at <a target="_blank" href="https://www.iros2017.org/">IROS 2017</a></h4>
                                                <p><b>[Oral]</b> Feeling the Force: Integrating Force and Pose for Fluent Discovery through Imitation Learning to Open Medicine Bottles.</p>
                                                <p>
                                                    [<a target="_blank" href="publications/iros2017openbottle/iros2017openbottle.pdf">paper</a>]
                                                    [<a target="_blank" href="https://vimeo.com/227504384">demo</a>]
                                                    [<a target="_blank" href="https://github.com/xiaozhuchacha/OpenBottle">code</a>]]
                                                    [<a target="_blank" href="http://www.yzhu.io/projects/iros17_openbottle/index.html">project</a>]
                                                </p>
                                                <p>
                                                    <span class="label label-success">Oral</span>&nbsp;
                                                    <span class="label label-primary">Conference</span>&nbsp;
                                                    <span class="label label-normal">Robotics</span>&nbsp;
                                                </p>
                                            </div>
                                        </div>
                                    </dd>
                                    <dd class="pos-right clearfix">
                                        <div class="circ"></div>
                                        <div class="time">2017 Apr</div>
                                        <div class="events">
                                            <div class="pull-left">
                                                <p class="events-object img-rounded"><i class="fa fa-user fa-lg"></i></p>
                                            </div>
                                            <div class="events-body">
                                                <h4 class="events-heading">Invited talk at <a target="_blank" href="http://www.virtualrealityla.com/">VRLA Expo 2017</a></h4>
                                                <p><b>[Invited talk]</b> I presented our work on "Examining Human Physical Judgments Across Virtual Gravity Fields" in VRLA 2017.</p>
                                                <p>
                                                    <span class="label label-primary">Invited Talk</span>&nbsp;
                                                    <span class="label label-normal">Virtual Reality</span>&nbsp;
                                                    <span class="label label-normal">Cognitive Science</span>&nbsp;
                                                </p>
                                            </div>
                                        </div>
                                    </dd>
                                </div>
                                <dt data-toggle="collapse" href="#news-2016" style="cursor: pointer">2016</dt>
                                <div id="news-2016" class="panel-collapse collapse in">
                                    <dd class="pos-left clearfix">
                                        <div class="circ"></div>
                                        <div class="time">2016 Nov</div>
                                        <div class="events">
                                            <div class="pull-left">
                                                <p class="events-object img-rounded"><i class="fa fa-file fa-lg"></i></p>
                                            </div>
                                            <div class="events-body">
                                                <h4 class="events-heading">One paper accepted at <a target="_blank" href="http://www.ieeevr.org/2017/">IEEE Virtual Reality 2017</a></h4>
                                                <p><b>[Oral]</b> The Martian: Examining Human Physical Judgments Across Virtual Gravity Fields.</p>
                                                <p>Accepted to <a target="_blank" href="https://www.computer.org/web/tvcg">TVCG</a></p>
                                                <p>
                                                    [<a target="_blank" href="publications/tvcg2016gravity/tvcg2016gravity.pdf">paper</a>]
                                                    [<a target="_blank" href="https://vimeo.com/195425617">demo</a>]
                                                    [<a target="_blank" href="http://www.yzhu.io/projects/tvcg16_gravity/index.html">project</a>]
                                                </p>
                                                <p>
                                                <span class="label label-success">Oral</span>&nbsp;
                                                <span class="label label-primary">Journal</span>&nbsp;
                                                <span class="label label-normal">Virtual Reality</span>&nbsp;
                                                <span class="label label-normal">Cognitive Science</span>&nbsp;
                                                </p>
                                            </div>
                                        </div>
                                    </dd>
                                </div>

                            </dl>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <!-- Projects Section -->
        <section id="projects" class="projects-section">
            <div class="container">
                <div class="row">
                    <div class="col-lg-1"></div>
                    <div class="col-lg-10">
                        <div class="section-title"><h2>Projects</h2></div>
                        <hr>
                    </div>
                    <div class="col-lg-1"></div>
                </div>

                <div class="row">
                    <div class="col-lg-1"></div>
                    <div class="col-lg-10">
                        <!--Put the contents here-->

                        <!--------------- Human Prediction --------------->
                        <div class="panel panel-default">
                            <div class="panel-body">
                                <ul class="media-list">
                                    <li class="media">
                                        <a class="pull-left" target="_blank" href=""><img
                                            class="events-object img-thumbnail img-rounded"
                                            src="projects/thumbnails/prediction.gif" width="150" height="150"></a>
                                        <div class="media-body">
                                            <h4 class="media-heading">Human Activity Prediction</h4>
                                            <p> This project aims to predict future human activities from partially
                                                observed RGB-D videos. Human activity prediction is generally difficult
                                                due to its non-Markovian property and the rich context between human and
                                                environments. We use a stochastic grammar model to capture the
                                                compositional/hierarchical structure of events, integrating human
                                                actions, objects, and their affordances. </p>

                                            <span class="label label-primary">Computer Vision</span>&nbsp;
                                            <span class="label label-primary">Robotics</span>&nbsp;
                                        </div>
                                    </li>
                                </ul>
                            </div>
                        </div>

                        <!--------------- Scene Synthesis --------------->
                        <div class="panel panel-default">
                            <div class="panel-body">
                                <ul class="media-list">
                                    <li class="media">
                                        <a class="pull-left" target="_blank" href=""><img
                                            class="events-object img-thumbnail img-rounded"
                                            src="projects/thumbnails/scenesynthesis.gif" width="150" height="150"></a>
                                        <div class="media-body">
                                            <h4 class="media-heading">Indoor Scene Synthesis by Stochastic Grammar</h4>
                                            <p>This project studies how to realistically synthesis indoor scene layouts
                                                using stochastic grammar. We present a novel human-centric method to
                                                sample 3D room layouts and synthesis photo-realistic images using
                                                physics-based rendering. We use object affordance and human activity
                                                planning to model indoor scenes, which contains functional grouping
                                                relations and supporting relations between furniture and objects. An
                                                attributed spatial And-Or graph (S-AOG) is proposed to model indoor
                                                scenes. The S-AOG is a stochastic context sensitive grammar, in which
                                                the terminal nodes are object entities including room, furniture and
                                                supported objects. </p>

                                            <span class="label label-primary">Computer Vision</span>&nbsp;
                                            <span class="label label-primary">Computer Graphics</span>&nbsp;
                                        </div>
                                    </li>
                                </ul>
                            </div>
                        </div>

                    </div>
                </div>
            </div>
        </section>

        <!-- Publications Section -->
        <section id="publications" class="publications-section">
            <div class="container">
                <div class="row">
                    <div class="col-lg-1"></div>
                    <div class="col-lg-10">
                        <div class="section-title"><h2>Publications</h2></div>
                        <hr>
                    </div>
                    <div class="col-lg-1"></div>
                </div>

                <div class="row">
                    <div class="col-lg-1"></div>
                    <div class="col-lg-10">

                        <!--------------- Human-centric Indoor Scene Synthesis --------------->
                        <div class="panel panel-default">
                            <!--<div class="panel-heading">-->
                            <!--<h3 class="panel-title">2016</h3>-->
                            <!--</div>-->
                            <div class="panel-body" data-toggle="collapse" href="#pub-abstract-cvpr2018synthesis" style="cursor: pointer">
                                <ul class="media-list">
                                    <li class="media">
                                        <a class="pull-left" target="_blank" href=""><img
                                            class="events-object img-thumbnail img-rounded"
                                            src="publications/thumbnails/cvpr2018synthesis.gif" width="150" height="150"></a>
                                        <div class="media-body">
                                            <h4 class="media-heading">Human-centric Indoor Scene Synthesis Using Stochastic Grammar</h4>
                                            <p><b>Siyuan Qi</b>, Yixin Zhu, Siyuan Huang, Chenfanfu Jiang, Song-Chun Zhu.</p>
                                            <p>CVPR 2018, Salt Lake City, USA</p>

                                            <span class="label label-primary">Conference</span>&nbsp;
                                            <span class="label label-normal">Computer Vision</span>&nbsp;
                                            <span class="label label-normal">Computer Graphics</span>&nbsp;
                                        </div>
                                    </li>

                                    <li>
                                        <div align="justify" id="pub-abstract-cvpr2018synthesis" class="panel-collapse collapse">
                                            <hr>
                                            <h5>Abstract</h5>
                                            We present a human-centric method to sample and synthesize 3D room layouts and 2D images thereof, for the purpose of obtaining large-scale 2D/3D image data with the perfect per-pixel ground truth.
                                            An attributed spatial And-Or graph (S-AOG) is proposed to represent indoor scenes. The S-AOG is a probabilistic grammar model, in which the terminal nodes are object entities including room, furniture, and supported objects. Human contexts as contextual relations are encoded by Markov Random Fields (MRF) on the terminal nodes.
                                            We learn the distributions from an indoor scene dataset and sample new layouts using Monte Carlo Markov Chain.
                                            Experiments demonstrate that the proposed method can robustly sample a large variety of realistic room layouts based on three criteria: (i) visual realism comparing to a state-of-the-art room arrangement method, (ii) accuracy of the affordance maps with respect to ground-truth, and (ii) the functionality and naturalness of synthesized rooms evaluated by human subjects..
                                        </div>
                                    </li>
                                </ul>
                            </div>

                            <ul class="list-group">
                                <li class="list-group-item">&nbsp;
                                    <span class="pull-right">
                                        <a target="_blank" data-toggle="collapse"
                                           title="bibtex" href="#pub-bibtex-cvpr2018synthesis">
                                            <i class="fa fa-pencil fa-lg"></i>
                                        </a>&nbsp;&nbsp;

                                        <a target="_blank" data-toggle="tooltip"
                                           title="Paper" href="publications/cvpr2018synthesis/cvpr2018synthesis.pdf">
                                            <i class="fa fa-file-pdf-o fa-lg"></i>
                                        </a>&nbsp;&nbsp

                                        <a target="_blank" data-toggle="tooltip"
                                           title="Supplementary" href="publications/cvpr2018synthesis/cvpr2018synthesis_supplementary.pdf">
                                            <i class="fa fa-file-pdf-o fa-lg"></i>
                                        </a>&nbsp;&nbsp

                                        <a target="_blank" data-toggle="tooltip"
                                           title="Code" href="https://github.com/SiyuanQi/human-centric-scene-synthesis">
                                            <i class="fa fa-code fa-lg"></i>
                                        </a>&nbsp;&nbsp;

                                        <a target="_blank" data-toggle="tooltip"
                                           title="Project page" href="http://www.yzhu.io/projects/cvpr18_scenesynthesis/index.html">
                                            <i class="fa fa-expand fa-lg"></i>
                                        </a>
                                    </span>
                                </li>
                            </ul>

                            <div align="justify" id="pub-bibtex-cvpr2018synthesis" class="panel-collapse collapse">
<pre>
@inproceedings{qi2018human,
    title={Human-centric Indoor Scene Synthesis Using Stochastic Grammar},
    author={Qi, Siyuan and Zhu, Yixin and Huang, Siyuan and Jiang, Chenfanfu and Zhu, Song-Chun},
    booktitle={Conference on Computer Vision and Pattern Recognition (CVPR)},
    year={2018}
}</pre>
                            </div>
                        </div>

                        <!--------------- Intent-aware Multi-agent Reinforcement Learning --------------->
                        <div class="panel panel-default">
                            <!--<div class="panel-heading">-->
                            <!--<h3 class="panel-title">2016</h3>-->
                            <!--</div>-->
                            <div class="panel-body" data-toggle="collapse" href="#pub-abstract-icra2018intent" style="cursor: pointer">
                                <ul class="media-list">
                                    <li class="media">
                                        <a class="pull-left" target="_blank" href=""><img
                                            class="events-object img-thumbnail img-rounded"
                                            src="publications/thumbnails/icra2018intent.gif" width="150" height="150"></a>
                                        <div class="media-body">
                                            <h4 class="media-heading">Intent-aware Multi-agent Reinforcement Learning</h4>
                                            <p><b>Siyuan Qi</b>, Song-Chun Zhu.</p>
                                            <p>ICRA 2018, Brisbane, Australia</p>

                                            <span class="label label-primary">Conference</span>&nbsp;
                                            <span class="label label-normal">Reinforcement Learning</span>&nbsp;
                                            <span class="label label-normal">Robotics</span>&nbsp;
                                        </div>
                                    </li>

                                    <li>
                                        <div align="justify" id="pub-abstract-icra2018intent" class="panel-collapse collapse">
                                            <hr>
                                            <h5>Abstract</h5>
                                            This paper proposes an intent-aware multi-agent planning framework as well as a learning algorithm. Under this framework, an agent plans in the goal space to maximize the expected utility. The planning process takes the belief of other agents' intents into consideration. Instead of formulating the learning problem as a partially observable Markov decision process (POMDP), we propose a simple but effective linear function approximation of the utility function. It is based on the observation that for humans, other people's intents will pose an influence on our utility for a goal. The proposed framework has several major advantages: i) it is computationally feasible and guaranteed to converge. ii) It can easily integrate existing intent prediction and low-level planning algorithms. iii) It does not suffer from sparse feedbacks in the action space. We experiment our algorithm in a real-world problem that is non-episodic, and the number of agents and goals can vary over time. Our algorithm is trained in a scene in which aerial robots and humans interact, and tested in a novel scene with a different environment. Experimental results show that our algorithm achieves the best performance and human-like behaviors emerge during the dynamic process.
                                        </div>
                                    </li>
                                </ul>
                            </div>

                            <ul class="list-group">
                                <li class="list-group-item">&nbsp;
                                    <span class="pull-right">
                                        <a target="_blank" data-toggle="collapse"
                                           title="bibtex" href="#pub-bibtex-icra2018intent">
                                            <i class="fa fa-pencil fa-lg"></i>
                                        </a>&nbsp;&nbsp;

                                        <a target="_blank" data-toggle="tooltip"
                                           title="Paper" href="publications/icra2018intent/icra2018intent.pdf">
                                            <i class="fa fa-file-pdf-o fa-lg"></i>
                                        </a>&nbsp;&nbsp

                                        <a target="_blank" data-toggle="tooltip"
                                           title="Demo" href="https://youtu.be/UtYbynV9sK0">
                                            <i class="fa fa-file-video-o fa-lg"></i>
                                        </a>&nbsp;&nbsp;

                                        <a target="_blank" data-toggle="tooltip"
                                           title="Code" href="https://github.com/SiyuanQi/intentMARL">
                                            <i class="fa fa-code fa-lg"></i>
                                        </a>&nbsp;&nbsp;
                                    </span>
                                </li>
                            </ul>

                            <div align="justify" id="pub-bibtex-icra2018intent" class="panel-collapse collapse">
<pre>
@inproceedings{qi2018intent,
    title={Intent-aware Multi-agent Reinforcement Learning},
    author={Qi, Siyuan and Zhu, Song-Chun},
    booktitle={International Conference on Robotics and Automation (ICRA)},
    year={2018}
}</pre>
                            </div>
                        </div>

                        <!--------------- Human Activity Prediction --------------->
                        <div class="panel panel-default">
                            <!--<div class="panel-heading">-->
                            <!--<h3 class="panel-title">2016</h3>-->
                            <!--</div>-->
                            <div class="panel-body" data-toggle="collapse" href="#pub-abstract-iccv2017prediction" style="cursor: pointer">
                                <ul class="media-list">
                                    <li class="media">
                                        <a class="pull-left" target="_blank" href=""><img
                                            class="events-object img-thumbnail img-rounded"
                                            src="publications/thumbnails/iccv2017prediction.gif" width="150" height="150"></a>
                                        <div class="media-body">
                                            <h4 class="media-heading">Predicting Human Activities Using Stochastic Grammar</h4>
                                            <p><b>Siyuan Qi</b>, Siyuan Huang, Ping Wei, Song-Chun Zhu.</p>
                                            <p>ICCV 2017, Venice, Italy</p>

                                            <span class="label label-primary">Conference</span>&nbsp;
                                            <span class="label label-normal">Computer Vision</span>&nbsp;
                                        </div>
                                    </li>

                                    <li>
                                        <div align="justify" id="pub-abstract-iccv2017prediction" class="panel-collapse collapse">
                                            <hr>
                                            <h5>Abstract</h5>
                                            This paper presents a novel method to predict future human activities from partially observed RGB-D videos. Human activity prediction is generally difficult due to its non-Markovian property and the rich context between human and environments.
                                            We use a stochastic grammar model to capture the compositional structure of events, integrating human actions, objects, and their affordances. We represent the event by a spatial-temporal And-Or graph (ST-AOG). The ST-AOG is composed of a temporal stochastic grammar defined on sub-activities, and spatial graphs representing sub-activities that consist of human actions, objects, and their affordances. Future sub-activities are predicted using the temporal grammar and Earley parsing algorithm. The corresponding action, object, and affordance labels are then inferred accordingly. Extensive experiments are conducted to show the effectiveness of our model on both semantic event parsing and future activity prediction.
                                        </div>
                                    </li>
                                </ul>
                            </div>

                            <ul class="list-group">
                                <li class="list-group-item">&nbsp;
                                    <span class="pull-right">
                                        <a target="_blank" data-toggle="collapse"
                                           title="bibtex" href="#pub-bibtex-iccv2017prediction">
                                            <i class="fa fa-pencil fa-lg"></i>
                                        </a>&nbsp;&nbsp;

                                        <a target="_blank" data-toggle="tooltip"
                                           title="Paper" href="publications/iccv2017prediction/iccv2017prediction.pdf">
                                            <i class="fa fa-file-pdf-o fa-lg"></i>
                                        </a>&nbsp;&nbsp

                                        <a target="_blank" data-toggle="tooltip"
                                           title="Demo" href="https://youtu.be/z2gwj8AnA_c">
                                            <i class="fa fa-file-video-o fa-lg"></i>
                                        </a>&nbsp;&nbsp;

                                        <a target="_blank" data-toggle="tooltip"
                                           title="Code" href="https://github.com/SiyuanQi/grammar-activity-prediction">
                                            <i class="fa fa-code fa-lg"></i>
                                        </a>&nbsp;&nbsp;
                                    </span>
                                </li>
                            </ul>

                            <div align="justify" id="pub-bibtex-iccv2017prediction" class="panel-collapse collapse">
<pre>
@inproceedings{qi2017predicting,
    title={Predicting Human Activities Using Stochastic Grammar},
    author={Qi, Siyuan and Huang, Siyuan and Wei, Ping and Zhu, Song-Chun},
    booktitle={International Conference on Computer Vision (ICCV)},
    year={2017}
}</pre>
                            </div>
                        </div>

                        <!--------------- Open bottle --------------->
                        <div class="panel panel-default">
                            <!--<div class="panel-heading">-->
                            <!--<h3 class="panel-title">2016</h3>-->
                            <!--</div>-->
                            <div class="panel-body" data-toggle="collapse" href="#pub-abstract-iros2017openbottle" style="cursor: pointer">
                                <ul class="media-list">
                                    <li class="media">
                                        <a class="pull-left" target="_blank" href="">
                                            <img class="events-object img-thumbnail img-rounded"
                                                 src="publications/thumbnails/iros2017openbottle.gif" width="150" height="150"><br>
                                            <img class="events-object img-thumbnail img-rounded"
                                                 src="publications/thumbnails/iros2017openbottle2.gif" width="150" height="150">
                                        </a>
                                        <div class="media-body">
                                            <h4 class="media-heading">Feeling  the  Force:  Integrating  Force  and  Pose  for  Fluent  Discovery through  Imitation  Learning  to  Open  Medicine  Bottles</h4>
                                            <p>Mark Edmonds*, Feng Gao*, Xu Xie, Hangxin Liu, <b>Siyuan Qi</b>, Yixin Zhu, Brandon Rothrock, Song-Chun Zhu. <br> * equal contributors</p>
                                            <p>IROS 2017, Vancouver, Canada</p>

                                            <span class="label label-success">Oral</span>&nbsp;
                                            <span class="label label-primary">Conference</span>&nbsp;
                                            <span class="label label-normal">Robotics</span>&nbsp;
                                        </div>
                                    </li>

                                    <li>
                                        <div align="justify" id="pub-abstract-iros2017openbottle" class="panel-collapse collapse">
                                            <hr>
                                            <h5>Abstract</h5>
                                            Learning complex robot manipulation policies for real-world objects is challenging, often requiring significant tuning within controlled environments. In this paper, we learn a manipulation model to execute tasks with multiple stages and variable structure, which typically are not suitable for most robot manipulation approaches. The model is learned from human demonstration using a tactile glove that measures both hand pose and contact forces. The tactile glove enables observation of visually latent changes in the scene, specifically the forces imposed to unlock the child-safety mechanisms of medicine bottles. From these observations, we learn an action planner through both a top-down stochastic grammar model (And-Or graph) to represent the compositional nature of the task sequence and a bottom-up discriminative model from the observed poses and forces. These two terms are combined during planning to select the next optimal action. We present a method for transferring this human-specific knowledge onto a robot platform and demonstrate that the robot can perform successful manipulations of unseen objects with similar task structure.
                                        </div>
                                    </li>
                                </ul>
                            </div>

                            <ul class="list-group">
                                <li class="list-group-item">&nbsp;
                                    <span class="pull-right">
                                        <a target="_blank" data-toggle="collapse"
                                           title="bibtex" href="#pub-bibtex-iros2017openbottle">
                                            <i class="fa fa-pencil fa-lg"></i>
                                        </a>&nbsp;&nbsp;

                                        <a target="_blank" data-toggle="tooltip"
                                           title="Paper" href="publications/iros2017openbottle/iros2017openbottle.pdf">
                                            <i class="fa fa-file-pdf-o fa-lg"></i>
                                        </a>&nbsp;&nbsp

                                        <a target="_blank" data-toggle="tooltip"
                                           title="Demo" href="https://vimeo.com/227504384">
                                            <i class="fa fa-file-video-o fa-lg"></i>
                                        </a>&nbsp;&nbsp;

                                        <a target="_blank" data-toggle="tooltip"
                                           title="Code" href="https://github.com/xiaozhuchacha/OpenBottle">
                                            <i class="fa fa-code fa-lg"></i>
                                        </a>&nbsp;&nbsp;

                                        <a target="_blank" data-toggle="tooltip"
                                           title="Project page" href="http://www.yzhu.io/projects/iros17_openbottle/index.html">
                                            <i class="fa fa-expand fa-lg"></i>
                                        </a>
                                    </span>
                                </li>
                            </ul>

                            <div align="justify" id="pub-bibtex-iros2017openbottle" class="panel-collapse collapse">
<pre>
@inproceedings{edmonds2017feeling,
    title={Feeling the Force: Integrating Force and Pose for Fluent Discovery through Imitation Learning to Open Medicine Bottles },
    author={Edmonds, Mark and Gao, Feng and Xie, Xu and Liu, Hangxin and Qi, Siyuan and Zhu, Yixin and Rothrock, Brandon and Zhu, Song-Chun},
    booktitle={International Conference on Intelligent Robots and Systems (IROS)},
    year={2017}
}</pre>
                            </div>
                        </div>

                        <!--------------- Arxiv Scene Synthesis --------------->
                        <div class="panel panel-default">
                            <!--<div class="panel-heading">-->
                            <!--<h3 class="panel-title">2016</h3>-->
                            <!--</div>-->
                            <div class="panel-body" data-toggle="collapse" href="#pub-abstract-scenesynthesis17arxiv" style="cursor: pointer">
                                <ul class="media-list">
                                    <li class="media">
                                        <a class="pull-left" target="_blank" href=""><img
                                            class="events-object img-thumbnail img-rounded"
                                            src="publications/thumbnails/scenesynthesis17arxiv.gif" width="150" height="150"></a>
                                        <div class="media-body">
                                            <h4 class="media-heading">Configurable, Photorealistic Image Rendering and Ground Truth Synthesis by Sampling Stochastic Grammars Representing Indoor Scenes</h4>
                                            <p>Chenfanfu Jiang*, <b>Siyuan Qi*</b>, Yixin Zhu*, Siyuan Huang*,
                                                Jenny Lin, Lap-Fai Yu, Demetri Terzopoulos, Song-Chun Zhu. <br> * equal contributors</p>
                                            <p>Under review for IJCV </p>

                                            <span class="label label-primary">Journal</span>&nbsp;
                                            <span class="label label-normal">Computer Vision</span>&nbsp;
                                            <span class="label label-normal">Computer Graphics</span>&nbsp;
                                        </div>
                                    </li>

                                    <li>
                                        <div align="justify" id="pub-abstract-scenesynthesis17arxiv" class="panel-collapse collapse">
                                            <hr>
                                            <h5>Abstract</h5>
                                            We propose a systematic learning-based approach to the generation of massive quantities of synthetic 3D scenes and numerous photorealistic 2D images thereof, with associated ground truth information, for the purposes of training, benchmarking, and diagnosing learning-based computer vision and robotics algorithms. In particular, we devise a learning-based pipeline of algorithms capable of automatically generating and rendering a potentially infinite variety of indoor scenes by using a stochastic grammar, represented as an attributed Spatial And-Or Graph, in conjunction with state-of-the-art physics-based rendering. Our pipeline is capable of synthesizing scene layouts with high diversity, and it is configurable in that it enables the precise customization and control of important attributes of the generated scenes. It renders photorealistic RGB images of the generated scenes while automatically synthesizing detailed, per-pixel ground truth data, including visible surface depth and normal, object identity, and material information (detailed to object parts), as well as environments (e.g., illumination and camera viewpoints). We demonstrate the value of our dataset, by improving performance in certain machine-learning-based scene understanding tasks--e.g., depth and surface normal prediction, semantic segmentation, reconstruction, etc.---and by providing benchmarks for and diagnostics of trained models by modifying object attributes and scene properties in a controllable manner.
                                        </div>
                                    </li>
                                </ul>
                            </div>

                            <ul class="list-group">
                                <li class="list-group-item">&nbsp;
                                    <span class="pull-right">
                                        <a target="_blank" data-toggle="tooltip"
                                           title="Paper" href="https://arxiv.org/abs/1704.00112">
                                            <i class="fa fa-file-pdf-o fa-lg"></i>
                                        </a>&nbsp;&nbsp;

                                        <a target="_blank" data-toggle="tooltip"
                                           title="Demo" href="https://vimeo.com/211226594">
                                            <i class="fa fa-file-video-o fa-lg"></i>
                                        </a>&nbsp;&nbsp;

                                        <a target="_blank" data-toggle="tooltip"
                                           title="Project page" href="http://www.yzhu.io/projects/arxiv_scenesynthesis/index.html">
                                            <i class="fa fa-expand fa-lg"></i>
                                        </a>
                                    </span>
                                </li>
                            </ul>
                        </div>

                        <!--------------- VR 2017 Gravity --------------->
                        <div class="panel panel-default">
                            <!--<div class="panel-heading">-->
                                <!--<h3 class="panel-title">2016</h3>-->
                            <!--</div>-->
                            <div class="panel-body" data-toggle="collapse" href="#pub-abstract-gravity16tvcg" style="cursor: pointer">
                                <ul class="media-list">
                                    <li class="media">
                                        <a class="pull-left" target="_blank" href=""><img
                                            class="events-object img-thumbnail img-rounded"
                                            src="publications/thumbnails/gravity16tvcg.gif" width="150" height="150"></a>
                                        <div class="media-body">
                                            <h4 class="media-heading"><b>[Oral]</b> The Martian: Examining Human
                                                Physical Judgments Across Virtual Gravity Fields.</h4>
                                            <p>Tian Ye*, <b>Siyuan Qi*</b>, James Kubricht, Yixin Zhu, Hongjing Lu,
                                                Song-Chun Zhu. <br> * equal contributors</p>
                                            <p>IEEE VR 2017, Los Angeles, California, USA <br> Accepted to TVCG</p>

                                            <span class="label label-success">Oral</span>&nbsp;
                                            <span class="label label-primary">Journal</span>&nbsp;
                                            <span class="label label-normal">Virtual Reality</span>&nbsp;
                                            <span class="label label-normal">Cognitive Science</span>&nbsp;
                                        </div>
                                    </li>

                                    <li>
                                        <div align="justify" id="pub-abstract-gravity16tvcg" class="panel-collapse collapse">
                                            <hr>
                                            <h5>Abstract</h5>
                                            This paper examines how humans adapt to novel physical situations with unknown gravitational acceleration in immersive virtual environments. We designed four virtual reality experiments with different tasks for participants to complete: strike a ball to hit a target, trigger a ball to hit a target, predict the landing location of a projectile, and estimate the flight duration of a projectile. The first two experiments compared human behavior in the virtual environment with real-world performance reported in the literature. The last two experiments aimed to test the human ability to adapt to novel gravity fields by measuring their performance in trajectory prediction and time estimation tasks. The experiment results show that: 1) based on brief observation of a projectile's initial trajectory, humans are accurate at predicting the landing location even under novel gravity fields, and 2) humans' time estimation in a familiar earth environment fluctuates around the ground truth flight duration, although the time estimation in unknown gravity fields indicates a bias toward earth's gravity.
                                        </div>
                                    </li>
                                </ul>
                            </div>

                            <ul class="list-group">
                                <li class="list-group-item">&nbsp;
                                    <span class="pull-right">

                                        <a target="_blank" data-toggle="collapse"
                                           title="bibtex" href="#pub-bibtex-gravity16tvcg">
                                            <i class="fa fa-pencil fa-lg"></i>
                                        </a>&nbsp;&nbsp;

                                        <a target="_blank" data-toggle="tooltip"
                                           title="Paper" href="publications/tvcg2016gravity/tvcg2016gravity.pdf">
                                            <i class="fa fa-file-pdf-o fa-lg"></i>
                                        </a>&nbsp;&nbsp;

                                        <a target="_blank" data-toggle="tooltip"
                                           title="Demo" href="https://vimeo.com/195425617">
                                            <i class="fa fa-file-video-o fa-lg"></i>
                                        </a>&nbsp;&nbsp;

                                        <a target="_blank" data-toggle="tooltip"
                                           title="Project page" href="http://www.yzhu.io/projects/tvcg16_gravity/index.html">
                                            <i class="fa fa-expand fa-lg"></i>
                                        </a>
                                    </span>
                                </li>
                            </ul>

                            <div align="justify" id="pub-bibtex-gravity16tvcg" class="panel-collapse collapse">
<pre>
@article{ye2017martian,
    title={The Martian: Examining Human Physical Judgments across Virtual Gravity Fields},
    author={Ye, Tian and Qi, Siyuan and Kubricht, James and Zhu, Yixin and Lu, Hongjing and Zhu, Song-Chun},
    journal={IEEE Transactions on Visualization and Computer Graphics},
    volume={23},
    number={4},
    pages={1399--1408},
    year={2017},
    publisher={IEEE}
}</pre>
                            </div>
                        </div>

                    </div>
                    <div class="col-lg-1"></div>
                </div>
            </div>
        </section>

        <!-- Contact Section -->
        <section id="more" class="more-section">
            <div class="container">
                <div class="row">
                    <div class="col-lg-1"></div>
                    <div class="col-lg-10">
                        <div class="section-title"><h2>More</h2></div>
                        <hr>

                        <div class="panel">
                            <ul id="more-tab" class="nav nav-tabs nav-justified">
                                <li class="active dropdown">
                                    <a href="#" id="more-tab-col-drop" class="dropdown-toggle" data-toggle="dropdown" style="cursor: pointer">Collaborators <b class="caret"></b></a>
                                    <ul class="dropdown-menu" role="menu" aria-labelledby="more-tab-col-drop">
                                        <li><a href="#more-col-vcla" tabindex="-1" data-toggle="tab">VCLA</a></li>
                                        <li><a href="#more-col-cross" tabindex="-1" data-toggle="tab">Cross-lab</a></li>
                                    </ul>
                                </li>
                                <!--<li><a href="#more-cv" data-toggle="tab">Curriculum Vitae</a></li>-->
                                <li><a href="#more-awards" data-toggle="tab">Awards</a></li>
                            </ul>
                            <div id="more-tab-content" class="tab-content">
                                <div class="tab-pane fade" id="more-col-vcla">
                                    <ul class="list-group">
                                        <li class="list-group-item">
                                            <a target="_blank" href="http://www.yzhu.io/index.html">Yixin Zhu</a> and
                                            <a target="_blank" href="https://thusiyuan.github.io/">Siyuan Huang</a> on Scene Generation and Synthesis.
                                        </li>
                                        <li class="list-group-item">
                                            <a target="_blank" href="http://www.stat.ucla.edu/~pwei/">Ping Wei</a> on Human Activity Understanding.
                                        </li>
                                        <li class="list-group-item">
                                            <a target="_blank" href="http://www.mjedmonds.com/">Mark Edmonds</a> and
                                            <a target="_blank" href="https://fen9.github.io/">Feng Gao</a> on Robot Learning.
                                        </li>
                                        <li class="list-group-item">
                                            <a target="_blank" href="http://jlin.crevado.com/">Jenny Lin</a>,
                                            <a target="_blank" href="https://xwguo.github.io/">Xingwen Guo</a> and Ye Tian on Virtual Reality.
                                        </li>
                                    </ul>
                                </div>
                                <div class="tab-pane fade active in" id="more-col-cross">
                                    <ul class="list-group">
                                        <li class="list-group-item">
                                            <a target="_blank" href="http://www.seas.upenn.edu/~cffjiang/">Dr. Chenfanfu Jiang</a> at
                                            <a target="_blank" href="http://cg.cis.upenn.edu/">UPenn Computer Graphics Group</a>
                                        </li>
                                        <li class="list-group-item">
                                            <a target="_blank" href="http://web.cs.ucla.edu/~dt/">Prof. Demetri Terzopoulos</a> at
                                            <a target="_blank" href="http://www.magix.ucla.edu/">UCLA Computer Graphics & Vision Laboratory</a>
                                        </li>
                                        <li class="list-group-item">
                                            Dr. Sara Spotorno,
                                            <a target="_blank" href="https://sites.google.com/site/skytianxu/">Tian Xu</a> and
                                            <a target="_blank" href="http://www.gla.ac.uk/researchinstitutes/neurosciencepsychology/staff/philippeschyns/">Prof. Philippe Schyns</a> at
                                            <a target="_blank" href="http://www.ccni.gla.ac.uk/">University of Glasgow, Centre for Cognitive Neuroimaging</a>
                                        </li>
                                        <li class="list-group-item">
                                            <a target="_blank" href="http://reasoninglab.psych.ucla.edu/Labbies/JamesKubricht.html">James Kubricht</a> and
                                            <a target="_blank" href="http://cvl.psych.ucla.edu/people.htm">Prof. Hongjing Lu</a> at
                                            <a target="_blank" href="http://cvl.psych.ucla.edu/">UCLA Computational Vision and Learning Lab (CVL)</a>
                                        </li>
                                        <li class="list-group-item">
                                            <a target="_blank" href="http://www.stat.ucla.edu/~ybzhao/">Dr. Yibiao Zhao</a> at
                                            <a target="_blank" href="http://cocosci.mit.edu/">MIT Computational Cognitive Science Group (cocosci)</a>
                                        </li>
                                        <li class="list-group-item">
                                            <a target="_blank" href="http://www.cs.umb.edu/~craigyu/">Lap-Fai (Craig) Yu</a> at University of Massachusetts Boston, Graphics and Virtual Environments Lab
                                        </li>
                                    </ul>
                                </div>
                                <!--<div class="tab-pane fade" id="more-cv">-->
                                <!--</div>-->
                                <div class="tab-pane fade" id="more-awards">
                                    <ul class="list-group">
                                        <li class="list-group-item">First Class Honors, Faculty of Engineering, University of Hong Kong <div class="pull-right">2013</div></li>
                                        <li class="list-group-item">Undergraduate Research Fellowship, University of Hong Kong <div class="pull-right">2012</div></li>
                                        <li class="list-group-item">Kingboard Scholarship, University of Hong Kong <div class="pull-right">2010 & 2011 & 2012</div></li>
                                        <li class="list-group-item">Dean's Honors List, University of Hong Kong <div class="pull-right">2010 & 2011</div></li>
                                        <li class="list-group-item">AI Challenge (Sponsored by Google), 2nd place in Chinese contestants, 74th worldwide <div class="pull-right">2011</div></li>
                                        <li class="list-group-item">Student Ambassador, University of Hong Kong <div class="pull-right">2010</div></li>
                                        <li class="list-group-item">University Entrance Scholarship, University of Hong Kong <div class="pull-right">2010</div></li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                    </div>
                    <div class="col-lg-1"></div>
                </div>
            </div>
        </section>

        <div class="footer text-center">
            <div class="footer-copyright text-center">Copyright &copy; 2017 Siyuan Qi</div>
        </div>


        <!-------------------- JavaScript and Google Analytics -------------------->
        <script src="https://code.jquery.com/jquery-3.2.0.min.js" integrity="sha256-JAW99MJVpJBGcbzEuXk4Az05s/XyDdBomFqNlM3ic+I=" crossorigin="anonymous"></script>
        <script>window.jQuery || document.write('<script src="js/vendor/jquery-3.2.0.min.js"><\/script>')</script>

        <!-- Bootstrap Core JavaScript -->
        <script src="js/vendor/bootstrap.min.js"></script>

        <!-- Bootflat's JS files.-->
        <script src="js/vendor/bootflat/icheck.min.js"></script>
        <script src="js/vendor/bootflat/jquery.fs.selecter.min.js"></script>
        <script src="js/vendor/bootflat/jquery.fs.stepper.min.js"></script>

        <!-- Scrolling Nav JavaScript -->
        <script src="js/vendor/jquery.easing.min.js"></script>
        <script src="js/scrolling-nav.js"></script>

        <!-- Custom JS files.-->
        <script src="js/plugins.js"></script>
        <script src="js/main.js"></script>

        <!-- Google Analytics: change UA-XXXXX-Y to be your site's ID. -->
        <script>
            if (window.location.host==="www.cs.ucla.edu" || window.location.host==="web.cs.ucla.edu" || window.location.host==="cs.ucla.edu" || window.location.host==="siyuanqi.github.io") {
                window.ga=function(){ga.q.push(arguments)};ga.q=[];ga.l=+new Date;
                ga('create','UA-83051006-1','auto');ga('send','pageview')
            }
        </script>
        <script src="https://www.google-analytics.com/analytics.js" async defer></script>
    </body>
</html>
