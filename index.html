<!doctype html>
<html class="no-js" lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <title>Siyuan Qi @ UCLA</title>
    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="shortcut icon" href="img/Q.jpg"/>
    <!-- Place favicon.ico in the root directory -->

    <link href="https://fonts.googleapis.com/css?family=Libre+Baskerville|Merriweather|Merriweather+Sans"
          rel="stylesheet">
    <link rel="stylesheet" href="css/animate.min.css">
    <link rel="stylesheet" href="css/normalize.css">
    <link rel="stylesheet" href="css/bootstrap.min.css">
    <link rel="stylesheet" href="css/bootflat.min.css">
    <link rel="stylesheet" href="css/font-awesome/css/font-awesome.css">
    <link rel="stylesheet" href="css/academicons/css/academicons.css">
    <link rel="stylesheet" href="css/main.css">
    <link rel="stylesheet" href="css/scrolling-nav.css">

    <script src="js/vendor/modernizr-2.8.3.min.js"></script>
    <meta name="google-site-verification" content="TZK0p3WighxuhGYfOu7bIxXGFLn0IAi0J8xF57zCMTY" />
</head>
<body class="animated fadeIn" id="page-top" data-spy="scroll" data-target=".navbar-fixed-top">
<!--[if lte IE 9]>
<p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade
    your browser</a> to improve your experience and security.</p>
<![endif]-->

<!-- Add your site or application content here -->
<!-- Navigation -->
<nav class="navbar navbar-default navbar-fixed-top" role="navigation"
     style="font-family: 'Merriweather Sans', sans-serif;">
    <div class="container">

        <div class="col-lg-8">
            <div class="navbar-header page-scroll">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a class="navbar-brand page-scroll" href="#page-top">Siyuan Qi</a>
            </div>

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse navbar-ex1-collapse">
                <ul class="nav navbar-nav">
                    <!-- Hidden li included to remove active class from about link when scrolled up past about section -->
                    <li class="hidden">
                        <a class="page-scroll" href="#page-top"></a>
                    </li>
                    <!--<li>-->
                    <!--<a class="page-scroll" href="#intro">Home</a>-->
                    <!--</li>-->
                    <li>
                        <a class="page-scroll" href="#about">About</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#news">News</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#projects">Projects</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#publications">Publications</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#more">More</a>
                    </li>
                </ul>
            </div>
            <!-- /.navbar-collapse -->
        </div>


        <div class="col-lg-4">
            <ul class="top-contact-info">
                <li><a target="_self" href="mailto:syqi@cs.ucla.edu"><i class="fa fa-envelope"></i></a></li>
                <li><a target="_blank" href="https://scholar.google.com/citations?hl=en&user=ePclJR4AAAAJ"><i
                    class="ai ai-google-scholar"></i></a></li>
                <li><a target="_blank" href="https://www.linkedin.com/in/siyuanqi"><i class="fa fa-linkedin"></i></a>
                </li>
                <li><a target="_blank" href="https://github.com/SiyuanQi"><i class="fa fa-github"></i></a></li>
                <li><a target="_blank" href="https://soundcloud.com/siyuanqi"><i class="fa fa-soundcloud"></i></a></li>
            </ul>
        </div>
    </div>
    <!-- /.container -->
</nav>

<!-- Intro Section -->
<section id="intro" class="intro-section">
    <div class="intro-profile">
        <div class="intro-avatar">
            <div class="section-title">
                <img class="img-circle" src="img/avatar.jpg" alt="" height="150">
            </div>
        </div>
        <div class="intro-info">
            <div class="section-title">
                <h3>Siyuan Qi</h3>
                <h5>PhD @ UCLA CS</h5>
            </div>
            <!-- <a class="btn btn-default page-scroll" href="#about">Click Me to Scroll Down!</a> -->
        </div>
    </div>
</section>

<!-- About Section -->
<section id="about" class="about-section">
    <div class="container">
        <div class="row">
            <div class="col-lg-1"></div>
            <div class="col-lg-10">
                <div class="section-title"><h3>About Me</h3></div>
                <p>I graduated from the <a href="http://www.cs.ucla.edu/">Computer Science
                    Department</a> at the <a href="http://www.ucla.edu/">University of California, Los Angeles</a> in
                    2019. During my Ph.D. study I did computer vision research in the <a href="http://vcla.stat.ucla.edu/" target="_blank">Center for Vision,
                    Cognition, Learning, and Autonomy</a> advised by <a href="http://www.stat.ucla.edu/~sczhu/" target="_blank">Professor Song-Chun Zhu</a>.</p>

                <p>I am currently working at Google. Please refer to <a
                    href="https://scholar.google.com/citations?user=ePclJR4AAAAJ" target="_blank">my Google Scholar
                    page</a> for a more up-to-date publication list.</p>

                <p>My research interests include Computer Vision, Machine Learning, and Cognitive Science.</p>
                <blockquote>
                    <p>We who cut mere stones must always be envisioning cathedrals.</p>
                    <footer>Quarry worker's creed</footer>
                </blockquote>
            </div>
            <div class="col-lg-1"></div>
        </div>
    </div>
</section>

<!-- News Section -->
<section id="news" class="news-section">
    <div class="container">
        <div class="row">
            <div class="col-lg-1"></div>
            <div class="col-lg-10">
                <div class="section-title"><h2>News</h2></div>
            </div>
            <div class="col-lg-1"></div>
        </div>

        <div class="row">
            <div class="col-md-12">
                <div class="timeline">
                    <dl>
                        <dt data-toggle="collapse" href="#news-2018" style="cursor: pointer">2018</dt>
                        <div id="news-2018" class="panel-collapse collapse in">
                            <dd class="pos-right clearfix">
                                <div class="circ"></div>
                                <div class="time">2018 Sep</div>
                                <div class="events">
                                    <div class="pull-left">
                                        <p class="events-object img-rounded"><i class="fa fa-file fa-lg"></i></p>
                                    </div>
                                    <div class="events-body">
                                        <h4 class="events-heading">One paper accepted at <a target="_blank"
                                                                                            href="https://neurips.cc/Conferences/2018">NeurIPS
                                            2018</a></h4>
                                        <p>Cooperative Holistic 3D Scene Understanding from a Single RGB Image.</p>
                                        <p>
                                            [<a target="_blank"
                                                href="publications/neurips2018cooperative/neurips2018cooperative.pdf">paper</a>]
                                            [<a target="_blank"
                                                href="publications/neurips2018cooperative/neurips2018cooperative_supplementary.pdf">supplementary</a>]
                                            <!--[<a target="_blank" href="https://github.com/SiyuanQi/generalized-earley-parser">code</a>]-->
                                        </p>
                                        <p>
                                            <span class="label label-primary">Conference</span>&nbsp;
                                            <span class="label label-normal">Machine Learning</span>&nbsp;
                                            <span class="label label-normal">Computer Vision</span>&nbsp;
                                        </p>
                                    </div>
                                </div>
                            </dd>
                            <dd class="pos-left clearfix">
                                <div class="circ"></div>
                                <div class="time">2018 Jul</div>
                                <div class="events">
                                    <div class="pull-left">
                                        <p class="events-object img-rounded"><i class="fa fa-file fa-lg"></i></p>
                                    </div>
                                    <div class="events-body">
                                        <h4 class="events-heading">Two papers accepted at <a target="_blank"
                                                                                             href="https://eccv2018.org/">ECCV
                                            2018</a></h4>
                                        <p>Learning Human-Object Interactions by Graph Parsing Neural Networks.</p>
                                        <p>
                                            [<a target="_blank"
                                                href="publications/eccv2018gpnn/eccv2018gpnn.pdf">paper</a>]
                                            [<a target="_blank" href="https://github.com/SiyuanQi/gpnn">code</a>]
                                        </p>
                                        <p>Holistic 3D Scene Parsing and Reconstruction from a Single RGB Image.</p>
                                        <p>
                                            [<a target="_blank" href="publications/eccv2018scene/eccv2018scene.pdf">paper</a>]
                                            [<a target="_blank"
                                                href="publications/eccv2018scene/eccv2018scene_supplementary.pdf">supplementary</a>]
                                            [<a target="_blank"
                                                href="https://github.com/thusiyuan/holistic_scene_parsing">code</a>]
                                        </p>
                                        <p>
                                            <span class="label label-primary">Conference</span>&nbsp;
                                            <span class="label label-normal">Computer Vision</span>&nbsp;
                                        </p>
                                    </div>
                                </div>
                            </dd>
                            <dd class="pos-right clearfix">
                                <div class="circ"></div>
                                <div class="time">2018 May</div>
                                <div class="events">
                                    <div class="pull-left">
                                        <p class="events-object img-rounded"><i class="fa fa-file fa-lg"></i></p>
                                    </div>
                                    <div class="events-body">
                                        <h4 class="events-heading">One paper accepted at <a target="_blank"
                                                                                            href="https://icml.cc/Conferences/2018">ICML
                                            2018</a></h4>
                                        <p>Generalized Earley Parser: Bridging Symbolic Grammars and Sequence Data for
                                            Future Prediction.</p>
                                        <p>
                                            [<a target="_blank" href="publications/icml2018earley/icml2018earley.pdf">paper</a>]
                                            [<a target="_blank"
                                                href="publications/icml2018earley/icml2018earley_supplementary.pdf">supplementary</a>]
                                            [<a target="_blank"
                                                href="https://github.com/SiyuanQi/generalized-earley-parser">code</a>]
                                        </p>
                                        <p>
                                            <span class="label label-primary">Conference</span>&nbsp;
                                            <span class="label label-normal">Machine Learning</span>&nbsp;
                                            <span class="label label-normal">Computer Vision</span>&nbsp;
                                        </p>
                                    </div>
                                </div>
                            </dd>
                            <dd class="pos-left clearfix">
                                <div class="circ"></div>
                                <div class="time">2018 May</div>
                                <div class="events">
                                    <div class="pull-left">
                                        <p class="events-object img-rounded"><i class="fa fa-file fa-lg"></i></p>
                                    </div>
                                    <div class="events-body">
                                        <h4 class="events-heading">One <a target="_blank"
                                                                          href="https://link.springer.com/journal/11263">IJCV</a>
                                            paper accepted.</h4>
                                        <p>Configurable 3D Scene Synthesis and 2D Image Rendering with Per-Pixel Ground
                                            Truth Using Stochastic Grammars.</p>
                                        <p>
                                            [<a target="_blank"
                                                href="publications/ijcv2018synthesis/ijcv2018synthesis.pdf">paper</a>]
                                            [<a target="_blank" href="https://vimeo.com/211226594">demo</a>]
                                            [<a target="_blank"
                                                href="https://github.com/SiyuanQi/human-centric-scene-synthesis">code</a>]
                                        </p>
                                        <p>
                                            <span class="label label-primary">Journal</span>&nbsp;
                                            <span class="label label-normal">Computer Vision</span>&nbsp;
                                            <span class="label label-normal">Computer Graphics</span>&nbsp;
                                        </p>
                                    </div>
                                </div>
                            </dd>
                            <dd class="pos-right clearfix">
                                <div class="circ"></div>
                                <div class="time">2018 Feb</div>
                                <div class="events">
                                    <div class="pull-left">
                                        <p class="events-object img-rounded"><i class="fa fa-file fa-lg"></i></p>
                                    </div>
                                    <div class="events-body">
                                        <h4 class="events-heading">One paper accepted at <a target="_blank"
                                                                                            href="http://cvpr2018.thecvf.com/">CVPR
                                            2018</a></h4>
                                        <p>Human-centric Indoor Scene Synthesis Using Stochastic Grammar.</p>
                                        <p>
                                            [<a target="_blank"
                                                href="publications/cvpr2018synthesis/cvpr2018synthesis.pdf">paper</a>]
                                            [<a target="_blank"
                                                href="publications/cvpr2018synthesis/cvpr2018synthesis_supplementary.pdf">supplementary</a>]
                                            [<a target="_blank"
                                                href="https://github.com/SiyuanQi/human-centric-scene-synthesis">code</a>]
                                            [<a target="_blank"
                                                href="http://www.yzhu.io/projects/cvpr18_scenesynthesis/index.html">project</a>]
                                        </p>
                                        <p>
                                            <span class="label label-primary">Conference</span>&nbsp;
                                            <span class="label label-normal">Computer Vision</span>&nbsp;
                                            <span class="label label-normal">Computer Graphics</span>&nbsp;
                                        </p>
                                    </div>
                                </div>
                            </dd>
                            <dd class="pos-left clearfix">
                                <div class="circ"></div>
                                <div class="time">2018 Jan</div>
                                <div class="events">
                                    <div class="pull-left">
                                        <p class="events-object img-rounded"><i class="fa fa-file fa-lg"></i></p>
                                    </div>
                                    <div class="events-body">
                                        <h4 class="events-heading">Two papers accepted at <a target="_blank"
                                                                                             href="https://icra2018.org/">ICRA
                                            2018</a></h4>
                                        <p>Intent-aware Multi-agent Reinforcement Learning.</p>
                                        <p>
                                            [<a target="_blank" href="publications/icra2018intent/icra2018intent.pdf">paper</a>]
                                            [<a target="_blank" href="https://youtu.be/UtYbynV9sK0">demo</a>]
                                            [<a target="_blank" href="https://github.com/SiyuanQi/intentMARL">code</a>]
                                        </p>
                                        <p>Unsupervised Learning of Hierarchical Models for Hand-Object Interactions
                                            Using Tactile Glove.</p>
                                        <p>
                                            [<a target="_blank"
                                                href="publications/icra2018gloveaction/icra2018gloveaction.pdf">paper</a>]
                                            [<a target="_blank" href="https://vimeo.com/259416784">demo</a>]
                                            [<a target="_blank"
                                                href="https://github.com/xuxie1031/UnsupervisedGloveAction">code</a>]
                                            [<a target="_blank"
                                                href="http://www.yzhu.io/projects/icra18_gloveaction/index.html">project</a>]
                                        </p>
                                        <p>
                                            <span class="label label-primary">Conference</span>&nbsp;
                                            <span class="label label-normal">Robotics</span>&nbsp;
                                        </p>
                                    </div>
                                </div>
                            </dd>
                        </div>
                        <dt data-toggle="collapse" href="#news-2017" style="cursor: pointer">2017</dt>
                        <div id="news-2017" class="panel-collapse collapse in">
                            <dd class="pos-right clearfix">
                                <div class="circ"></div>
                                <div class="time">2017 Jul</div>
                                <div class="events">
                                    <div class="pull-left">
                                        <p class="events-object img-rounded"><i class="fa fa-file fa-lg"></i></p>
                                    </div>
                                    <div class="events-body">
                                        <h4 class="events-heading">One paper accepted at <a target="_blank"
                                                                                            href="http://iccv2017.thecvf.com/">ICCV
                                            2017</a></h4>
                                        <p>Predicting Human Activities Using Stochastic Grammar.</p>
                                        <p>
                                            [<a target="_blank"
                                                href="publications/iccv2017prediction/iccv2017prediction.pdf">paper</a>]
                                            [<a target="_blank" href="https://youtu.be/z2gwj8AnA_c">demo</a>]
                                            [<a target="_blank"
                                                href="https://github.com/SiyuanQi/grammar-activity-prediction">code</a>]
                                        </p>
                                        <p>
                                            <span class="label label-primary">Conference</span>&nbsp;
                                            <span class="label label-normal">Computer Vision</span>&nbsp;
                                        </p>
                                    </div>
                                </div>
                            </dd>
                            <dd class="pos-left clearfix">
                                <div class="circ"></div>
                                <div class="time">2017 Jun</div>
                                <div class="events">
                                    <div class="pull-left">
                                        <p class="events-object img-rounded"><i class="fa fa-file fa-lg"></i></p>
                                    </div>
                                    <div class="events-body">
                                        <h4 class="events-heading">One paper accepted at <a target="_blank"
                                                                                            href="https://www.iros2017.org/">IROS
                                            2017</a></h4>
                                        <p>Feeling the Force: Integrating Force and Pose for Fluent Discovery through
                                            Imitation Learning to Open Medicine Bottles.</p>
                                        <p>
                                            [<a target="_blank"
                                                href="publications/iros2017openbottle/iros2017openbottle.pdf">paper</a>]
                                            [<a target="_blank" href="https://vimeo.com/227504384">demo</a>]
                                            [<a target="_blank"
                                                href="https://github.com/xiaozhuchacha/OpenBottle">code</a>]
                                            [<a target="_blank"
                                                href="http://www.yzhu.io/projects/iros17_openbottle/index.html">project</a>]
                                        </p>
                                        <p>
                                            <!--<span class="label label-success">Oral</span>&nbsp;-->
                                            <span class="label label-primary">Conference</span>&nbsp;
                                            <span class="label label-normal">Robotics</span>&nbsp;
                                        </p>
                                    </div>
                                </div>
                            </dd>
                            <dd class="pos-right clearfix">
                                <div class="circ"></div>
                                <div class="time">2017 Apr</div>
                                <div class="events">
                                    <div class="pull-left">
                                        <p class="events-object img-rounded"><i class="fa fa-user fa-lg"></i></p>
                                    </div>
                                    <div class="events-body">
                                        <h4 class="events-heading">Invited talk at <a target="_blank"
                                                                                      href="http://www.virtualrealityla.com/">VRLA
                                            Expo 2017</a></h4>
                                        <p><b>[Invited talk]</b> I presented our work on "Examining Human Physical
                                            Judgments Across Virtual Gravity Fields" in VRLA 2017.</p>
                                        <p>
                                            <span class="label label-primary">Invited Talk</span>&nbsp;
                                            <span class="label label-normal">Virtual Reality</span>&nbsp;
                                            <span class="label label-normal">Cognitive Science</span>&nbsp;
                                        </p>
                                    </div>
                                </div>
                            </dd>
                        </div>
                        <dt data-toggle="collapse" href="#news-2016" style="cursor: pointer">2016</dt>
                        <div id="news-2016" class="panel-collapse collapse in">
                            <dd class="pos-left clearfix">
                                <div class="circ"></div>
                                <div class="time">2016 Nov</div>
                                <div class="events">
                                    <div class="pull-left">
                                        <p class="events-object img-rounded"><i class="fa fa-file fa-lg"></i></p>
                                    </div>
                                    <div class="events-body">
                                        <h4 class="events-heading">One paper accepted at <a target="_blank"
                                                                                            href="http://www.ieeevr.org/2017/">IEEE
                                            Virtual Reality 2017</a></h4>
                                        <p><b>[Oral]</b> The Martian: Examining Human Physical Judgments Across Virtual
                                            Gravity Fields.</p>
                                        <p>Accepted to <a target="_blank"
                                                          href="https://www.computer.org/web/tvcg">TVCG</a></p>
                                        <p>
                                            [<a target="_blank" href="publications/tvcg2016gravity/tvcg2016gravity.pdf">paper</a>]
                                            [<a target="_blank" href="https://vimeo.com/195425617">demo</a>]
                                            [<a target="_blank"
                                                href="http://www.yzhu.io/projects/tvcg16_gravity/index.html">project</a>]
                                        </p>
                                        <p>
                                            <span class="label label-success">Oral</span>&nbsp;
                                            <span class="label label-primary">Journal</span>&nbsp;
                                            <span class="label label-normal">Virtual Reality</span>&nbsp;
                                            <span class="label label-normal">Cognitive Science</span>&nbsp;
                                        </p>
                                    </div>
                                </div>
                            </dd>
                        </div>

                    </dl>
                </div>
            </div>
        </div>
    </div>
</section>

<!-- Projects Section -->
<section id="projects" class="projects-section">
    <div class="container">
        <div class="row">
            <div class="col-lg-1"></div>
            <div class="col-lg-10">
                <div class="section-title"><h2>Projects</h2></div>
                <hr>
            </div>
            <div class="col-lg-1"></div>
        </div>

        <div class="row">
            <div class="col-lg-1"></div>
            <div class="col-lg-10">
                <!--Put the contents here-->

                <!--------------- Human Prediction --------------->
                <div class="panel panel-default">
                    <div class="panel-body">
                        <ul class="media-list">
                            <li class="media">
                                <a class="pull-left" target="_blank" href=""><img
                                    class="events-object img-thumbnail img-rounded"
                                    src="projects/thumbnails/prediction.gif" width="150" height="150"></a>
                                <div class="media-body">
                                    <h4 class="media-heading">Human Activity Prediction</h4>
                                    <p> This project aims to predict future human activities from partially
                                        observed RGB-D videos. Human activity prediction is generally difficult
                                        due to its non-Markovian property and the rich context between human and
                                        environments. We use a stochastic grammar model to capture the
                                        compositional/hierarchical structure of events, integrating human
                                        actions, objects, and their affordances. </p>

                                    <span class="label label-primary">Computer Vision</span>&nbsp;
                                    <span class="label label-primary">Robotics</span>&nbsp;
                                </div>
                            </li>
                        </ul>
                    </div>
                </div>

                <!--------------- Scene Synthesis --------------->
                <div class="panel panel-default">
                    <div class="panel-body">
                        <ul class="media-list">
                            <li class="media">
                                <a class="pull-left" target="_blank" href=""><img
                                    class="events-object img-thumbnail img-rounded"
                                    src="projects/thumbnails/scenesynthesis.gif" width="150" height="150"></a>
                                <div class="media-body">
                                    <h4 class="media-heading">Indoor Scene Synthesis by Stochastic Grammar</h4>
                                    <p>This project studies how to realistically synthesis indoor scene layouts
                                        using stochastic grammar. We present a novel human-centric method to
                                        sample 3D room layouts and synthesis photo-realistic images using
                                        physics-based rendering. We use object affordance and human activity
                                        planning to model indoor scenes, which contains functional grouping
                                        relations and supporting relations between furniture and objects. An
                                        attributed spatial And-Or graph (S-AOG) is proposed to model indoor
                                        scenes. The S-AOG is a stochastic context sensitive grammar, in which
                                        the terminal nodes are object entities including room, furniture and
                                        supported objects. </p>

                                    <span class="label label-primary">Computer Vision</span>&nbsp;
                                    <span class="label label-primary">Computer Graphics</span>&nbsp;
                                </div>
                            </li>
                        </ul>
                    </div>
                </div>

            </div>
        </div>
    </div>
</section>

<!-- Publications Section -->
<section id="publications" class="publications-section">
    <div class="container">
        <div class="row">
            <div class="col-lg-1"></div>
            <div class="col-lg-10">
                <div class="section-title"><h2>Publications</h2></div>
                <hr>
            </div>
            <div class="col-lg-1"></div>
        </div>

        <div class="row">
            <div class="col-lg-1"></div>
            <div class="col-lg-10">

                <!--------------- Cooperative Scene Parsing --------------->
                <div class="panel panel-default">
                    <!--<div class="panel-heading">-->
                    <!--<h3 class="panel-title">2016</h3>-->
                    <!--</div>-->
                    <div class="panel-body" data-toggle="collapse" href="#pub-abstract-neurips2018cooperative"
                         style="cursor: pointer">
                        <ul class="media-list">
                            <li class="media">
                                <a class="pull-left" target="_blank" href=""><img
                                    class="events-object img-thumbnail img-rounded"
                                    src="publications/thumbnails/neurips2018cooperative.png" width="150"
                                    height="150"></a>
                                <div class="media-body">
                                    <h4 class="media-heading">Cooperative Holistic Scene Understanding: Unifying
                                        3D Object, Layout, and Camera Pose Estimation</h4>
                                    <p>Siyuan Huang, <b>Siyuan Qi</b>, Yinxue Xiao, Yixin Zhu, Ying Nian Wu, Song-Chun
                                        Zhu.</p>
                                    <p>neurips 2018, Montreal, Canada</p>

                                    <span class="label label-primary">Conference</span>&nbsp;
                                    <span class="label label-normal">Machine Learning</span>&nbsp;
                                    <span class="label label-normal">Computer Vision</span>&nbsp;
                                </div>
                            </li>

                            <li>
                                <div align="justify" id="pub-abstract-neurips2018cooperative"
                                     class="panel-collapse collapse">
                                    <hr>
                                    <h5>Abstract</h5>
                                    Holistic 3D indoor scene understanding refers to jointly recovering the i) object
                                    bounding boxes, ii) room layout, and iii) camera pose, all in 3D. The existing
                                    methods either are ineffective or only tackle the problem partially. In this paper,
                                    we propose an end-to-end model that simultaneously solves all three tasks in
                                    realtime
                                    given only a single RGB image. The essence of the proposed method is to
                                    improve the prediction by i) parametrizing the targets (e.g., 3D boxes) instead of
                                    directly estimating the targets, and ii) cooperative training across different
                                    modules
                                    in contrast to training these modules individually. Specifically, we parametrize
                                    the 3D object bounding boxes by the predictions from several modules, i.e., 3D
                                    camera pose and object attributes. The proposed method provides two major
                                    advantages: i) The parametrization helps maintain the consistency between the
                                    2D image and the 3D world, thus largely reducing the prediction variances in
                                    3D coordinates. ii) Constraints can be imposed on the parametrization to train
                                    different modules simultaneously. We call these constraints "cooperative losses" as
                                    they enable the joint training and inference. We employ three cooperative losses
                                    for 3D bounding boxes, 2D projections, and physical constraints to estimate a
                                    geometrically consistent and physically plausible 3D scene. Experiments on the
                                    SUN RGB-D dataset shows that the proposed method significantly outperforms
                                    prior approaches on 3D object detection, 3D layout estimation, 3D camera pose
                                    estimation, and holistic scene understanding.
                                </div>
                            </li>
                        </ul>
                    </div>

                    <ul class="list-group">
                        <li class="list-group-item">&nbsp;
                            <span class="pull-right">
                                        <a target="_blank" data-toggle="collapse"
                                           title="bibtex" href="#pub-bibtex-neurips2018cooperative">
                                            <i class="fa fa-pencil fa-lg"></i>
                                        </a>&nbsp;&nbsp;

                                        <a target="_blank" data-toggle="tooltip"
                                           title="Paper"
                                           href="publications/neurips2018cooperative/neurips2018cooperative.pdf">
                                            <i class="fa fa-file-pdf-o fa-lg"></i>
                                        </a>&nbsp;&nbsp

                                        <a target="_blank" data-toggle="tooltip"
                                           title="Supplementary"
                                           href="publications/neurips2018cooperative/neurips2018cooperative_supplementary.pdf">
                                            <i class="fa fa-file-pdf-o fa-lg"></i>
                                        </a>&nbsp;&nbsp
                                    </span>
                        </li>
                    </ul>

                    <div align="justify" id="pub-bibtex-neurips2018cooperative" class="panel-collapse collapse">
<pre>
@inproceedings{huang2018cooperative,
    title={Cooperative Holistic Scene Understanding: Unifying
3D Object, Layout, and Camera Pose Estimation},
    author={Huang, Siyuan and Qi, Siyuan and Xiao, Yinxue and Zhu, Yixin and Wu, Ying Nian and Zhu, Song-Chun},
    booktitle={Conference on Neural Information Processing Systems (NeurIPS)},
    year={2018}
}</pre>
                    </div>
                </div>

                <!--------------- Graph Parsing Neural Network --------------->
                <div class="panel panel-default">
                    <!--<div class="panel-heading">-->
                    <!--<h3 class="panel-title">2016</h3>-->
                    <!--</div>-->
                    <div class="panel-body" data-toggle="collapse" href="#pub-abstract-eccv2018gpnn"
                         style="cursor: pointer">
                        <ul class="media-list">
                            <li class="media">
                                <a class="pull-left" target="_blank" href=""><img
                                    class="events-object img-thumbnail img-rounded"
                                    src="publications/thumbnails/eccv2018gpnn.png" width="150" height="150"></a>
                                <div class="media-body">
                                    <h4 class="media-heading">Learning Human-Object Interactions by Graph Parsing Neural
                                        Networks</h4>
                                    <p><b>Siyuan Qi*</b>, Wenguan Wang*, Baoxiong Jia, Jianbing Shen, Song-Chun Zhu.</p>
                                    <p>ECCV 2018, Munich, Germany</p>

                                    <span class="label label-primary">Conference</span>&nbsp;
                                    <span class="label label-normal">Computer Vision</span>&nbsp;
                                </div>
                            </li>

                            <li>
                                <div align="justify" id="pub-abstract-eccv2018gpnn" class="panel-collapse collapse">
                                    <hr>
                                    <h5>Abstract</h5>
                                    This paper addresses the task of detecting and recognizing human-object interactions
                                    (HOI) in images and videos.
                                    We introduce the Graph Parsing Neural Network (GPNN), a framework that incorporates
                                    structural knowledge while being differentiable end-to-end.
                                    For a given scene, GPNN infers a parse graph that includes i) the HOI graph
                                    structure represented by an adjacency matrix, and ii) the node labels.
                                    Within a message passing inference framework, GPNN iteratively computes the
                                    adjacency matrices and node labels.
                                    We extensively evaluate our model on three HOI detection benchmarks on images and
                                    videos: HICO-DET, V-COCO, and CAD-120 datasets.
                                    Our approach significantly outperforms state-of-art methods, verifying that GPNN is
                                    scalable to large datasets and applies to spatial-temporal settings.
                                </div>
                            </li>
                        </ul>
                    </div>

                    <ul class="list-group">
                        <li class="list-group-item">&nbsp;
                            <span class="pull-right">
                                        <a target="_blank" data-toggle="collapse"
                                           title="bibtex" href="#pub-bibtex-eccv2018gpnn">
                                            <i class="fa fa-pencil fa-lg"></i>
                                        </a>&nbsp;&nbsp;

                                        <a target="_blank" data-toggle="tooltip"
                                           title="Paper" href="publications/eccv2018gpnn/eccv2018gpnn.pdf">
                                            <i class="fa fa-file-pdf-o fa-lg"></i>
                                        </a>&nbsp;&nbsp

                                        <a target="_blank" data-toggle="tooltip"
                                           title="Code" href="https://github.com/SiyuanQi/gpnn">
                                            <i class="fa fa-code fa-lg"></i>
                                        </a>&nbsp;&nbsp;
                                    </span>
                        </li>
                    </ul>

                    <div align="justify" id="pub-bibtex-eccv2018gpnn" class="panel-collapse collapse">
<pre>
@inproceedings{qi2018learning,
    title={Learning Human-Object Interactions by Graph Parsing Neural Networks},
    author={Qi, Siyuan and Wang, Wenguan and Jia, Baoxiong and Shen, Jianbing and Zhu, Song-Chun},
    booktitle={European Conference on Computer Vision (ECCV)},
    year={2018}
}</pre>
                    </div>
                </div>

                <!--------------- Holistic Scene Grammar --------------->
                <div class="panel panel-default">
                    <!--<div class="panel-heading">-->
                    <!--<h3 class="panel-title">2016</h3>-->
                    <!--</div>-->
                    <div class="panel-body" data-toggle="collapse" href="#pub-abstract-eccv2018scene"
                         style="cursor: pointer">
                        <ul class="media-list">
                            <li class="media">
                                <a class="pull-left" target="_blank" href=""><img
                                    class="events-object img-thumbnail img-rounded"
                                    src="publications/thumbnails/eccv2018scene.png" width="150" height="150"></a>
                                <div class="media-body">
                                    <h4 class="media-heading">Holistic 3D Scene Parsing and Reconstruction from a Single
                                        RGB Image</h4>
                                    <p>Siyuan Huang, <b>Siyuan Qi</b>, Yixin Zhu, Yinxue Xiao, Yuanlu Xu, Song-Chun Zhu.
                                    </p>
                                    <p>ECCV 2018, Munich, Germany</p>

                                    <span class="label label-primary">Conference</span>&nbsp;
                                    <span class="label label-normal">Computer Vision</span>&nbsp;
                                </div>
                            </li>

                            <li>
                                <div align="justify" id="pub-abstract-eccv2018scene" class="panel-collapse collapse">
                                    <hr>
                                    <h5>Abstract</h5>
                                    We propose a computational framework to jointly parse a
                                    single RGB image and reconstruct a holistic 3D configuration composed
                                    by a set of CAD models using a stochastic grammar model. Specifically,
                                    we introduce a Holistic Scene Grammar (HSG) to represent the 3D scene
                                    structure, which characterizes a joint distribution over the functional and
                                    geometric space of indoor scenes. The proposed HSG captures three essential
                                    and often latent dimensions of the indoor scenes: i) latent human
                                    context, describing the affordance and the functionality of a room arrangement,
                                    ii) geometric constraints over the scene configurations, and
                                    iii) physical constraints that guarantee physically plausible parsing and
                                    reconstruction. We solve this joint parsing and reconstruction problem
                                    in an analysis-by-synthesis fashion, seeking to minimize the differences
                                    between the input image and the rendered images generated by our 3D
                                    representation, over the space of depth, surface normal, and object segmentation
                                    map. The optimal configuration, represented by a parse graph,
                                    is inferred using Markov chain Monte Carlo (MCMC), which efficiently
                                    traverses through the non-differentiable solution space, jointly optimizing
                                    object localization, 3D layout, and hidden human context. Experimental
                                    results demonstrate that the proposed algorithm improves the generalization
                                    ability and significantly outperforms prior methods on 3D layout
                                    estimation, 3D object detection, and holistic scene understanding.
                                </div>
                            </li>
                        </ul>
                    </div>

                    <ul class="list-group">
                        <li class="list-group-item">&nbsp;
                            <span class="pull-right">
                                        <a target="_blank" data-toggle="collapse"
                                           title="bibtex" href="#pub-bibtex-eccv2018scene">
                                            <i class="fa fa-pencil fa-lg"></i>
                                        </a>&nbsp;&nbsp;

                                        <a target="_blank" data-toggle="tooltip"
                                           title="Paper" href="publications/eccv2018scene/eccv2018scene.pdf">
                                            <i class="fa fa-file-pdf-o fa-lg"></i>
                                        </a>&nbsp;&nbsp

                                        <a target="_blank" data-toggle="tooltip"
                                           title="Supplementary"
                                           href="publications/eccv2018scene/eccv2018scene_supplementary.pdf">
                                            <i class="fa fa-file-pdf-o fa-lg"></i>
                                        </a>&nbsp;&nbsp

                                        <a target="_blank" data-toggle="tooltip"
                                           title="Code" href="https://github.com/thusiyuan/holistic_scene_parsing">
                                            <i class="fa fa-code fa-lg"></i>
                                        </a>&nbsp;&nbsp;
                                    </span>
                        </li>
                    </ul>

                    <div align="justify" id="pub-bibtex-eccv2018scene" class="panel-collapse collapse">
<pre>
@inproceedings{huang2018holistic,
    title={Holistic 3D Scene Parsing and Reconstruction from a Single RGB Image},
    author={Huang, Siyuan and Qi, Siyuan and Zhu, Yixin and Xiao, Yinxue and Xu, Yuanlu and Zhu, Song-Chun},
    booktitle={European Conference on Computer Vision (ECCV)},
    year={2018}
}</pre>
                    </div>
                </div>

                <!--------------- Generalized Earley Parser --------------->
                <div class="panel panel-default">
                    <!--<div class="panel-heading">-->
                    <!--<h3 class="panel-title">2016</h3>-->
                    <!--</div>-->
                    <div class="panel-body" data-toggle="collapse" href="#pub-abstract-icml2018earley"
                         style="cursor: pointer">
                        <ul class="media-list">
                            <li class="media">
                                <a class="pull-left" target="_blank" href=""><img
                                    class="events-object img-thumbnail img-rounded"
                                    src="publications/thumbnails/icml2018earley.jpg" width="150" height="150"></a>
                                <div class="media-body">
                                    <h4 class="media-heading">Generalized Earley Parser: Bridging Symbolic Grammars and
                                        Sequence Data for Future Prediction</h4>
                                    <p><b>Siyuan Qi</b>, Baoxiong Jia, Song-Chun Zhu.</p>
                                    <p>ICML 2018, Stockholm, Sweden</p>

                                    <span class="label label-primary">Conference</span>&nbsp;
                                    <span class="label label-normal">Machine Learning</span>&nbsp;
                                    <span class="label label-normal">Computer Vision</span>&nbsp;
                                </div>
                            </li>

                            <li>
                                <div align="justify" id="pub-abstract-icml2018earley" class="panel-collapse collapse">
                                    <hr>
                                    <h5>Abstract</h5>
                                    Future predictions on sequence data (e.g., videos or audios) require the algorithms
                                    to capture non-Markovian and compositional properties of high-level semantics.
                                    Context-free grammars are natural choices to capture such properties, but
                                    traditional grammar parsers (e.g., Earley parser) only take symbolic sentences as
                                    input.
                                    In this paper, we generalize the Earley parser to parse sequence data which is
                                    neither segmented nor labeled.
                                    This generalized Earley parser integrates a grammar parser with a classifier to find
                                    the optimal segmentation and labels, and makes top-down future predictions
                                    accordingly.
                                    Experiments show that our method significantly outperforms other approaches for
                                    future human activity prediction.
                                </div>
                            </li>
                        </ul>
                    </div>

                    <ul class="list-group">
                        <li class="list-group-item">&nbsp;
                            <span class="pull-right">
                                        <a target="_blank" data-toggle="collapse"
                                           title="bibtex" href="#pub-bibtex-icml2018earley">
                                            <i class="fa fa-pencil fa-lg"></i>
                                        </a>&nbsp;&nbsp;

                                        <a target="_blank" data-toggle="tooltip"
                                           title="Paper" href="publications/icml2018earley/icml2018earley.pdf">
                                            <i class="fa fa-file-pdf-o fa-lg"></i>
                                        </a>&nbsp;&nbsp

                                        <a target="_blank" data-toggle="tooltip"
                                           title="Supplementary"
                                           href="publications/icml2018earley/icml2018earley_supplementary.pdf">
                                            <i class="fa fa-file-pdf-o fa-lg"></i>
                                        </a>&nbsp;&nbsp

                                        <a target="_blank" data-toggle="tooltip"
                                           title="Code" href="https://github.com/SiyuanQi/generalized-earley-parser">
                                            <i class="fa fa-code fa-lg"></i>
                                        </a>&nbsp;&nbsp;
                                    </span>
                        </li>
                    </ul>

                    <div align="justify" id="pub-bibtex-icml2018earley" class="panel-collapse collapse">
<pre>
@inproceedings{qi2018generalized,
    title={Generalized Earley Parser: Bridging Symbolic Grammars and Sequence Data for Future Prediction},
    author={Qi, Siyuan and Jia, Baoxiong and Zhu, Song-Chun},
    booktitle={International Conference on Machine Learning (ICML)},
    year={2018}
}</pre>
                    </div>
                </div>


                <!--------------- IJCV Scene Synthesis --------------->
                <div class="panel panel-default">
                    <!--<div class="panel-heading">-->
                    <!--<h3 class="panel-title">2016</h3>-->
                    <!--</div>-->
                    <div class="panel-body" data-toggle="collapse" href="#pub-abstract-ijcv2018synthesis"
                         style="cursor: pointer">
                        <ul class="media-list">
                            <li class="media">
                                <a class="pull-left" target="_blank" href=""><img
                                    class="events-object img-thumbnail img-rounded"
                                    src="publications/thumbnails/ijcv2018synthesis.gif" width="150" height="150"></a>
                                <div class="media-body">
                                    <h4 class="media-heading">Configurable 3D Scene Synthesis and 2D Image Rendering
                                        with Per-Pixel Ground Truth Using Stochastic Grammars</h4>
                                    <p>Chenfanfu Jiang*, <b>Siyuan Qi*</b>, Yixin Zhu*, Siyuan Huang*,
                                        Jenny Lin, Lap-Fai Yu, Demetri Terzopoulos, Song-Chun Zhu.</p>
                                    <p>IJCV 2018</p>

                                    <span class="label label-primary">Journal</span>&nbsp;
                                    <span class="label label-normal">Computer Vision</span>&nbsp;
                                    <span class="label label-normal">Computer Graphics</span>&nbsp;
                                </div>
                            </li>

                            <li>
                                <div align="justify" id="pub-abstract-ijcv2018synthesis"
                                     class="panel-collapse collapse">
                                    <hr>
                                    <h5>Abstract</h5>
                                    We propose a systematic learning-based approach to the generation of massive
                                    quantities of synthetic 3D scenes and numerous photorealistic 2D images thereof,
                                    with associated ground truth information, for the purposes of training,
                                    benchmarking, and diagnosing learning-based computer vision and robotics algorithms.
                                    In particular, we devise a learning-based pipeline of algorithms capable of
                                    automatically generating and rendering a potentially infinite variety of indoor
                                    scenes by using a stochastic grammar, represented as an attributed Spatial And-Or
                                    Graph, in conjunction with state-of-the-art physics-based rendering. Our pipeline is
                                    capable of synthesizing scene layouts with high diversity, and it is configurable in
                                    that it enables the precise customization and control of important attributes of the
                                    generated scenes. It renders photorealistic RGB images of the generated scenes while
                                    automatically synthesizing detailed, per-pixel ground truth data, including visible
                                    surface depth and normal, object identity, and material information (detailed to
                                    object parts), as well as environments (e.g., illumination and camera viewpoints).
                                    We demonstrate the value of our dataset, by improving performance in certain
                                    machine-learning-based scene understanding tasks--e.g., depth and surface normal
                                    prediction, semantic segmentation, reconstruction, etc.---and by providing
                                    benchmarks for and diagnostics of trained models by modifying object attributes and
                                    scene properties in a controllable manner.
                                </div>
                            </li>
                        </ul>
                    </div>
                    <ul class="list-group">
                        <li class="list-group-item">&nbsp;
                            <span class="pull-right">
                                        <a target="_blank" data-toggle="collapse"
                                           title="bibtex" href="#pub-bibtex-ijcv2018synthesis">
                                            <i class="fa fa-pencil fa-lg"></i>
                                        </a>&nbsp;&nbsp;

                                        <a target="_blank" data-toggle="tooltip"
                                           title="Paper" href="publications/ijcv2018synthesis/ijcv2018synthesis.pdf">
                                            <i class="fa fa-file-pdf-o fa-lg"></i>
                                        </a>&nbsp;&nbsp;

                                        <a target="_blank" data-toggle="tooltip"
                                           title="Demo" href="https://vimeo.com/211226594">
                                            <i class="fa fa-file-video-o fa-lg"></i>
                                        </a>&nbsp;&nbsp;

                                        <a target="_blank" data-toggle="tooltip"
                                           title="Project page"
                                           href="http://www.yzhu.io/projects/arxiv_scenesynthesis/index.html">
                                            <i class="fa fa-expand fa-lg"></i>
                                        </a>
                                    </span>
                        </li>
                    </ul>

                    <div align="justify" id="pub-bibtex-ijcv2018synthesis" class="panel-collapse collapse">
<pre>
@article{jiang2018configurable,
    title={Configurable 3D Scene Synthesis and 2D Image Rendering with Per-Pixel Ground Truth Using Stochastic Grammars},
    author={Jiang, Chenfanfu and Qi, Siyuan and Zhu, Yixin and Huang, Siyuan and Lin, Jenny and Yu, Lap-Fai and Terzopoulos, Demetri, Zhu, Song-Chun},
    journal = {International Journal of Computer Vision (IJCV)},
    year={2018}
}</pre>
                    </div>
                </div>

                <!--------------- Human-centric Indoor Scene Synthesis --------------->
                <div class="panel panel-default">
                    <!--<div class="panel-heading">-->
                    <!--<h3 class="panel-title">2016</h3>-->
                    <!--</div>-->
                    <div class="panel-body" data-toggle="collapse" href="#pub-abstract-cvpr2018synthesis"
                         style="cursor: pointer">
                        <ul class="media-list">
                            <li class="media">
                                <a class="pull-left" target="_blank" href=""><img
                                    class="events-object img-thumbnail img-rounded"
                                    src="publications/thumbnails/cvpr2018synthesis.gif" width="150" height="150"></a>
                                <div class="media-body">
                                    <h4 class="media-heading">Human-centric Indoor Scene Synthesis Using Stochastic
                                        Grammar</h4>
                                    <p><b>Siyuan Qi</b>, Yixin Zhu, Siyuan Huang, Chenfanfu Jiang, Song-Chun Zhu.</p>
                                    <p>CVPR 2018, Salt Lake City, USA</p>

                                    <span class="label label-primary">Conference</span>&nbsp;
                                    <span class="label label-normal">Computer Vision</span>&nbsp;
                                    <span class="label label-normal">Computer Graphics</span>&nbsp;
                                </div>
                            </li>

                            <li>
                                <div align="justify" id="pub-abstract-cvpr2018synthesis"
                                     class="panel-collapse collapse">
                                    <hr>
                                    <h5>Abstract</h5>
                                    We present a human-centric method to sample and synthesize 3D room layouts and 2D
                                    images thereof, for the purpose of obtaining large-scale 2D/3D image data with the
                                    perfect per-pixel ground truth.
                                    An attributed spatial And-Or graph (S-AOG) is proposed to represent indoor scenes.
                                    The S-AOG is a probabilistic grammar model, in which the terminal nodes are object
                                    entities including room, furniture, and supported objects. Human contexts as
                                    contextual relations are encoded by Markov Random Fields (MRF) on the terminal
                                    nodes.
                                    We learn the distributions from an indoor scene dataset and sample new layouts using
                                    Monte Carlo Markov Chain.
                                    Experiments demonstrate that the proposed method can robustly sample a large variety
                                    of realistic room layouts based on three criteria: (i) visual realism comparing to a
                                    state-of-the-art room arrangement method, (ii) accuracy of the affordance maps with
                                    respect to ground-truth, and (ii) the functionality and naturalness of synthesized
                                    rooms evaluated by human subjects.
                                </div>
                            </li>
                        </ul>
                    </div>

                    <ul class="list-group">
                        <li class="list-group-item">&nbsp;
                            <span class="pull-right">
                                        <a target="_blank" data-toggle="collapse"
                                           title="bibtex" href="#pub-bibtex-cvpr2018synthesis">
                                            <i class="fa fa-pencil fa-lg"></i>
                                        </a>&nbsp;&nbsp;

                                        <a target="_blank" data-toggle="tooltip"
                                           title="Paper" href="publications/cvpr2018synthesis/cvpr2018synthesis.pdf">
                                            <i class="fa fa-file-pdf-o fa-lg"></i>
                                        </a>&nbsp;&nbsp

                                        <a target="_blank" data-toggle="tooltip"
                                           title="Supplementary"
                                           href="publications/cvpr2018synthesis/cvpr2018synthesis_supplementary.pdf">
                                            <i class="fa fa-file-pdf-o fa-lg"></i>
                                        </a>&nbsp;&nbsp

                                        <a target="_blank" data-toggle="tooltip"
                                           title="Code"
                                           href="https://github.com/SiyuanQi/human-centric-scene-synthesis">
                                            <i class="fa fa-code fa-lg"></i>
                                        </a>&nbsp;&nbsp;

                                        <a target="_blank" data-toggle="tooltip"
                                           title="Project page"
                                           href="http://www.yzhu.io/projects/cvpr18_scenesynthesis/index.html">
                                            <i class="fa fa-expand fa-lg"></i>
                                        </a>
                                    </span>
                        </li>
                    </ul>

                    <div align="justify" id="pub-bibtex-cvpr2018synthesis" class="panel-collapse collapse">
<pre>
@inproceedings{qi2018human,
    title={Human-centric Indoor Scene Synthesis Using Stochastic Grammar},
    author={Qi, Siyuan and Zhu, Yixin and Huang, Siyuan and Jiang, Chenfanfu and Zhu, Song-Chun},
    booktitle={Conference on Computer Vision and Pattern Recognition (CVPR)},
    year={2018}
}</pre>
                    </div>
                </div>

                <!--------------- Intent-aware Multi-agent Reinforcement Learning --------------->
                <div class="panel panel-default">
                    <!--<div class="panel-heading">-->
                    <!--<h3 class="panel-title">2016</h3>-->
                    <!--</div>-->
                    <div class="panel-body" data-toggle="collapse" href="#pub-abstract-icra2018intent"
                         style="cursor: pointer">
                        <ul class="media-list">
                            <li class="media">
                                <a class="pull-left" target="_blank" href=""><img
                                    class="events-object img-thumbnail img-rounded"
                                    src="publications/thumbnails/icra2018intent.gif" width="150" height="150"></a>
                                <div class="media-body">
                                    <h4 class="media-heading">Intent-aware Multi-agent Reinforcement Learning</h4>
                                    <p><b>Siyuan Qi</b>, Song-Chun Zhu.</p>
                                    <p>ICRA 2018, Brisbane, Australia</p>

                                    <span class="label label-primary">Conference</span>&nbsp;
                                    <span class="label label-normal">Reinforcement Learning</span>&nbsp;
                                    <span class="label label-normal">Robotics</span>&nbsp;
                                </div>
                            </li>

                            <li>
                                <div align="justify" id="pub-abstract-icra2018intent" class="panel-collapse collapse">
                                    <hr>
                                    <h5>Abstract</h5>
                                    This paper proposes an intent-aware multi-agent planning framework as well as a
                                    learning algorithm. Under this framework, an agent plans in the goal space to
                                    maximize the expected utility. The planning process takes the belief of other
                                    agents' intents into consideration. Instead of formulating the learning problem as a
                                    partially observable Markov decision process (POMDP), we propose a simple but
                                    effective linear function approximation of the utility function. It is based on the
                                    observation that for humans, other people's intents will pose an influence on our
                                    utility for a goal. The proposed framework has several major advantages: i) it is
                                    computationally feasible and guaranteed to converge. ii) It can easily integrate
                                    existing intent prediction and low-level planning algorithms. iii) It does not
                                    suffer from sparse feedbacks in the action space. We experiment our algorithm in a
                                    real-world problem that is non-episodic, and the number of agents and goals can vary
                                    over time. Our algorithm is trained in a scene in which aerial robots and humans
                                    interact, and tested in a novel scene with a different environment. Experimental
                                    results show that our algorithm achieves the best performance and human-like
                                    behaviors emerge during the dynamic process.
                                </div>
                            </li>
                        </ul>
                    </div>

                    <ul class="list-group">
                        <li class="list-group-item">&nbsp;
                            <span class="pull-right">
                                        <a target="_blank" data-toggle="collapse"
                                           title="bibtex" href="#pub-bibtex-icra2018intent">
                                            <i class="fa fa-pencil fa-lg"></i>
                                        </a>&nbsp;&nbsp;

                                        <a target="_blank" data-toggle="tooltip"
                                           title="Paper" href="publications/icra2018intent/icra2018intent.pdf">
                                            <i class="fa fa-file-pdf-o fa-lg"></i>
                                        </a>&nbsp;&nbsp

                                        <a target="_blank" data-toggle="tooltip"
                                           title="Demo" href="https://youtu.be/UtYbynV9sK0">
                                            <i class="fa fa-file-video-o fa-lg"></i>
                                        </a>&nbsp;&nbsp;

                                        <a target="_blank" data-toggle="tooltip"
                                           title="Code" href="https://github.com/SiyuanQi/intentMARL">
                                            <i class="fa fa-code fa-lg"></i>
                                        </a>&nbsp;&nbsp;
                                    </span>
                        </li>
                    </ul>

                    <div align="justify" id="pub-bibtex-icra2018intent" class="panel-collapse collapse">
<pre>
@inproceedings{qi2018intent,
    title={Intent-aware Multi-agent Reinforcement Learning},
    author={Qi, Siyuan and Zhu, Song-Chun},
    booktitle={International Conference on Robotics and Automation (ICRA)},
    year={2018}
}</pre>
                    </div>
                </div>

                <!--------------- Unsupervised Learning of Hierarchical Models for Hand-Object Interactions --------------->
                <div class="panel panel-default">
                    <!--<div class="panel-heading">-->
                    <!--<h3 class="panel-title">2016</h3>-->
                    <!--</div>-->
                    <div class="panel-body" data-toggle="collapse" href="#pub-abstract-icra2018gloveaction"
                         style="cursor: pointer">
                        <ul class="media-list">
                            <li class="media">
                                <a class="pull-left" target="_blank" href=""><img
                                    class="events-object img-thumbnail img-rounded"
                                    src="publications/thumbnails/icra2018gloveaction.png" width="150" height="150"></a>
                                <div class="media-body">
                                    <h4 class="media-heading">Unsupervised Learning of Hierarchical Models for
                                        Hand-Object Interactions</h4>
                                    <p>Xu Xie, Hangxin Liu, Mark Edmonds, Feng Gao, <b>Siyuan Qi</b>, Yixin Zhu, Brandon
                                        Rothrock, Song-Chun Zhu</p>
                                    <p>ICRA 2018, Brisbane, Australia</p>

                                    <span class="label label-primary">Conference</span>&nbsp;
                                    <span class="label label-normal">Robotics</span>&nbsp;
                                </div>
                            </li>

                            <li>
                                <div align="justify" id="pub-abstract-icra2018gloveaction"
                                     class="panel-collapse collapse">
                                    <hr>
                                    <h5>Abstract</h5>
                                    Contact forces of the hand are visually unobservable, but play a crucial role in
                                    understanding hand-object interactions. In this paper, we propose an unsupervised
                                    learning approach for manipulation event segmentation and manipulation event
                                    parsing. The proposed framework incorporates hand pose kinematics and contact forces
                                    using a low-cost easy-to-replicate tactile glove. We use a temporal grammar model to
                                    capture the hierarchical structure of events, integrating extracted force vectors
                                    from the raw sensory input of poses and forces. The temporal grammar is represented
                                    as a temporal And-Or graph (T-AOG), which can be induced in an unsupervised manner.
                                    We obtain the event labeling sequences by measuring the similarity between segments
                                    using the Dynamic Time Alignment Kernel (DTAK). Experimental results show that our
                                    method achieves high accuracy in manipulation event segmentation, recognition and
                                    parsing by utilizing both pose and force data.
                                </div>
                            </li>
                        </ul>
                    </div>

                    <ul class="list-group">
                        <li class="list-group-item">&nbsp;
                            <span class="pull-right">
                                        <a target="_blank" data-toggle="collapse"
                                           title="bibtex" href="#pub-bibtex-icra2018gloveaction">
                                            <i class="fa fa-pencil fa-lg"></i>
                                        </a>&nbsp;&nbsp;

                                        <a target="_blank" data-toggle="tooltip"
                                           title="Paper"
                                           href="publications/icra2018gloveaction/icra2018gloveaction.pdf">
                                            <i class="fa fa-file-pdf-o fa-lg"></i>
                                        </a>&nbsp;&nbsp

                                        <a target="_blank" data-toggle="tooltip"
                                           title="Demo" href="https://vimeo.com/259416784">
                                            <i class="fa fa-file-video-o fa-lg"></i>
                                        </a>&nbsp;&nbsp;

                                        <a target="_blank" data-toggle="tooltip"
                                           title="Code" href="https://github.com/xuxie1031/UnsupervisedGloveAction">
                                            <i class="fa fa-code fa-lg"></i>
                                        </a>&nbsp;&nbsp;

                                        <a target="_blank" data-toggle="tooltip"
                                           title="Project page"
                                           href="http://www.yzhu.io/projects/icra18_gloveaction/index.html">
                                            <i class="fa fa-expand fa-lg"></i>
                                        </a>
                                    </span>
                        </li>
                    </ul>

                    <div align="justify" id="pub-bibtex-icra2018gloveaction" class="panel-collapse collapse">
<pre>
@inproceedings{xu2018unsupervised,
    title={Unsupervised Learning of Hierarchical Models for Hand-Object Interactions},
    author={Xie, Xu and Liu, Hangxin and Edmonds, Mark and Gao, Feng and Qi, Siyuan and Zhu, Yixin and Rothrock, Brandon and Zhu, Song-Chun},
    booktitle={International Conference on Robotics and Automation (ICRA)},
    year={2018}
}</pre>
                    </div>
                </div>

                <!--------------- Human Activity Prediction --------------->
                <div class="panel panel-default">
                    <!--<div class="panel-heading">-->
                    <!--<h3 class="panel-title">2016</h3>-->
                    <!--</div>-->
                    <div class="panel-body" data-toggle="collapse" href="#pub-abstract-iccv2017prediction"
                         style="cursor: pointer">
                        <ul class="media-list">
                            <li class="media">
                                <a class="pull-left" target="_blank" href=""><img
                                    class="events-object img-thumbnail img-rounded"
                                    src="publications/thumbnails/iccv2017prediction.gif" width="150" height="150"></a>
                                <div class="media-body">
                                    <h4 class="media-heading">Predicting Human Activities Using Stochastic Grammar</h4>
                                    <p><b>Siyuan Qi</b>, Siyuan Huang, Ping Wei, Song-Chun Zhu.</p>
                                    <p>ICCV 2017, Venice, Italy</p>

                                    <span class="label label-primary">Conference</span>&nbsp;
                                    <span class="label label-normal">Computer Vision</span>&nbsp;
                                </div>
                            </li>

                            <li>
                                <div align="justify" id="pub-abstract-iccv2017prediction"
                                     class="panel-collapse collapse">
                                    <hr>
                                    <h5>Abstract</h5>
                                    This paper presents a novel method to predict future human activities from partially
                                    observed RGB-D videos. Human activity prediction is generally difficult due to its
                                    non-Markovian property and the rich context between human and environments.
                                    We use a stochastic grammar model to capture the compositional structure of events,
                                    integrating human actions, objects, and their affordances. We represent the event by
                                    a spatial-temporal And-Or graph (ST-AOG). The ST-AOG is composed of a temporal
                                    stochastic grammar defined on sub-activities, and spatial graphs representing
                                    sub-activities that consist of human actions, objects, and their affordances. Future
                                    sub-activities are predicted using the temporal grammar and Earley parsing
                                    algorithm. The corresponding action, object, and affordance labels are then inferred
                                    accordingly. Extensive experiments are conducted to show the effectiveness of our
                                    model on both semantic event parsing and future activity prediction.
                                </div>
                            </li>
                        </ul>
                    </div>

                    <ul class="list-group">
                        <li class="list-group-item">&nbsp;
                            <span class="pull-right">
                                        <a target="_blank" data-toggle="collapse"
                                           title="bibtex" href="#pub-bibtex-iccv2017prediction">
                                            <i class="fa fa-pencil fa-lg"></i>
                                        </a>&nbsp;&nbsp;

                                        <a target="_blank" data-toggle="tooltip"
                                           title="Paper" href="publications/iccv2017prediction/iccv2017prediction.pdf">
                                            <i class="fa fa-file-pdf-o fa-lg"></i>
                                        </a>&nbsp;&nbsp

                                        <a target="_blank" data-toggle="tooltip"
                                           title="Demo" href="https://youtu.be/z2gwj8AnA_c">
                                            <i class="fa fa-file-video-o fa-lg"></i>
                                        </a>&nbsp;&nbsp;

                                        <a target="_blank" data-toggle="tooltip"
                                           title="Code" href="https://github.com/SiyuanQi/grammar-activity-prediction">
                                            <i class="fa fa-code fa-lg"></i>
                                        </a>&nbsp;&nbsp;
                                    </span>
                        </li>
                    </ul>

                    <div align="justify" id="pub-bibtex-iccv2017prediction" class="panel-collapse collapse">
<pre>
@inproceedings{qi2017predicting,
    title={Predicting Human Activities Using Stochastic Grammar},
    author={Qi, Siyuan and Huang, Siyuan and Wei, Ping and Zhu, Song-Chun},
    booktitle={International Conference on Computer Vision (ICCV)},
    year={2017}
}</pre>
                    </div>
                </div>

                <!--------------- Open bottle --------------->
                <div class="panel panel-default">
                    <!--<div class="panel-heading">-->
                    <!--<h3 class="panel-title">2016</h3>-->
                    <!--</div>-->
                    <div class="panel-body" data-toggle="collapse" href="#pub-abstract-iros2017openbottle"
                         style="cursor: pointer">
                        <ul class="media-list">
                            <li class="media">
                                <a class="pull-left" target="_blank" href="">
                                    <img class="events-object img-thumbnail img-rounded"
                                         src="publications/thumbnails/iros2017openbottle.gif" width="150"
                                         height="150"><br>
                                    <img class="events-object img-thumbnail img-rounded"
                                         src="publications/thumbnails/iros2017openbottle2.gif" width="150" height="150">
                                </a>
                                <div class="media-body">
                                    <h4 class="media-heading">Feeling the Force: Integrating Force and Pose for Fluent
                                        Discovery through Imitation Learning to Open Medicine Bottles</h4>
                                    <p>Mark Edmonds*, Feng Gao*, Xu Xie, Hangxin Liu, <b>Siyuan Qi</b>, Yixin Zhu,
                                        Brandon Rothrock, Song-Chun Zhu.</p>
                                    <p>IROS 2017, Vancouver, Canada</p>

                                    <span class="label label-success">Oral</span>&nbsp;
                                    <span class="label label-primary">Conference</span>&nbsp;
                                    <span class="label label-normal">Robotics</span>&nbsp;
                                </div>
                            </li>

                            <li>
                                <div align="justify" id="pub-abstract-iros2017openbottle"
                                     class="panel-collapse collapse">
                                    <hr>
                                    <h5>Abstract</h5>
                                    Learning complex robot manipulation policies for real-world objects is challenging,
                                    often requiring significant tuning within controlled environments. In this paper, we
                                    learn a manipulation model to execute tasks with multiple stages and variable
                                    structure, which typically are not suitable for most robot manipulation approaches.
                                    The model is learned from human demonstration using a tactile glove that measures
                                    both hand pose and contact forces. The tactile glove enables observation of visually
                                    latent changes in the scene, specifically the forces imposed to unlock the
                                    child-safety mechanisms of medicine bottles. From these observations, we learn an
                                    action planner through both a top-down stochastic grammar model (And-Or graph) to
                                    represent the compositional nature of the task sequence and a bottom-up
                                    discriminative model from the observed poses and forces. These two terms are
                                    combined during planning to select the next optimal action. We present a method for
                                    transferring this human-specific knowledge onto a robot platform and demonstrate
                                    that the robot can perform successful manipulations of unseen objects with similar
                                    task structure.
                                </div>
                            </li>
                        </ul>
                    </div>

                    <ul class="list-group">
                        <li class="list-group-item">&nbsp;
                            <span class="pull-right">
                                        <a target="_blank" data-toggle="collapse"
                                           title="bibtex" href="#pub-bibtex-iros2017openbottle">
                                            <i class="fa fa-pencil fa-lg"></i>
                                        </a>&nbsp;&nbsp;

                                        <a target="_blank" data-toggle="tooltip"
                                           title="Paper" href="publications/iros2017openbottle/iros2017openbottle.pdf">
                                            <i class="fa fa-file-pdf-o fa-lg"></i>
                                        </a>&nbsp;&nbsp

                                        <a target="_blank" data-toggle="tooltip"
                                           title="Demo" href="https://vimeo.com/227504384">
                                            <i class="fa fa-file-video-o fa-lg"></i>
                                        </a>&nbsp;&nbsp;

                                        <a target="_blank" data-toggle="tooltip"
                                           title="Code" href="https://github.com/xiaozhuchacha/OpenBottle">
                                            <i class="fa fa-code fa-lg"></i>
                                        </a>&nbsp;&nbsp;

                                        <a target="_blank" data-toggle="tooltip"
                                           title="Project page"
                                           href="http://www.yzhu.io/projects/iros17_openbottle/index.html">
                                            <i class="fa fa-expand fa-lg"></i>
                                        </a>
                                    </span>
                        </li>
                    </ul>

                    <div align="justify" id="pub-bibtex-iros2017openbottle" class="panel-collapse collapse">
<pre>
@inproceedings{edmonds2017feeling,
    title={Feeling the Force: Integrating Force and Pose for Fluent Discovery through Imitation Learning to Open Medicine Bottles },
    author={Edmonds, Mark and Gao, Feng and Xie, Xu and Liu, Hangxin and Qi, Siyuan and Zhu, Yixin and Rothrock, Brandon and Zhu, Song-Chun},
    booktitle={International Conference on Intelligent Robots and Systems (IROS)},
    year={2017}
}</pre>
                    </div>
                </div>

                <!--------------- VR 2017 Gravity --------------->
                <div class="panel panel-default">
                    <!--<div class="panel-heading">-->
                    <!--<h3 class="panel-title">2016</h3>-->
                    <!--</div>-->
                    <div class="panel-body" data-toggle="collapse" href="#pub-abstract-gravity16tvcg"
                         style="cursor: pointer">
                        <ul class="media-list">
                            <li class="media">
                                <a class="pull-left" target="_blank" href=""><img
                                    class="events-object img-thumbnail img-rounded"
                                    src="publications/thumbnails/gravity16tvcg.gif" width="150" height="150"></a>
                                <div class="media-body">
                                    <h4 class="media-heading"><b>[Oral]</b> The Martian: Examining Human
                                        Physical Judgments Across Virtual Gravity Fields.</h4>
                                    <p>Tian Ye, <b>Siyuan Qi</b>, James Kubricht, Yixin Zhu, Hongjing Lu,
                                        Song-Chun Zhu.</p>
                                    <p>IEEE VR 2017, Los Angeles, California, USA <br> Accepted to TVCG</p>

                                    <span class="label label-success">Oral</span>&nbsp;
                                    <span class="label label-primary">Journal</span>&nbsp;
                                    <span class="label label-normal">Virtual Reality</span>&nbsp;
                                    <span class="label label-normal">Cognitive Science</span>&nbsp;
                                </div>
                            </li>

                            <li>
                                <div align="justify" id="pub-abstract-gravity16tvcg" class="panel-collapse collapse">
                                    <hr>
                                    <h5>Abstract</h5>
                                    This paper examines how humans adapt to novel physical situations with unknown
                                    gravitational acceleration in immersive virtual environments. We designed four
                                    virtual reality experiments with different tasks for participants to complete:
                                    strike a ball to hit a target, trigger a ball to hit a target, predict the landing
                                    location of a projectile, and estimate the flight duration of a projectile. The
                                    first two experiments compared human behavior in the virtual environment with
                                    real-world performance reported in the literature. The last two experiments aimed to
                                    test the human ability to adapt to novel gravity fields by measuring their
                                    performance in trajectory prediction and time estimation tasks. The experiment
                                    results show that: 1) based on brief observation of a projectile's initial
                                    trajectory, humans are accurate at predicting the landing location even under novel
                                    gravity fields, and 2) humans' time estimation in a familiar earth environment
                                    fluctuates around the ground truth flight duration, although the time estimation in
                                    unknown gravity fields indicates a bias toward earth's gravity.
                                </div>
                            </li>
                        </ul>
                    </div>

                    <ul class="list-group">
                        <li class="list-group-item">&nbsp;
                            <span class="pull-right">

                                        <a target="_blank" data-toggle="collapse"
                                           title="bibtex" href="#pub-bibtex-gravity16tvcg">
                                            <i class="fa fa-pencil fa-lg"></i>
                                        </a>&nbsp;&nbsp;

                                        <a target="_blank" data-toggle="tooltip"
                                           title="Paper" href="publications/tvcg2016gravity/tvcg2016gravity.pdf">
                                            <i class="fa fa-file-pdf-o fa-lg"></i>
                                        </a>&nbsp;&nbsp;

                                        <a target="_blank" data-toggle="tooltip"
                                           title="Demo" href="https://vimeo.com/195425617">
                                            <i class="fa fa-file-video-o fa-lg"></i>
                                        </a>&nbsp;&nbsp;

                                        <a target="_blank" data-toggle="tooltip"
                                           title="Project page"
                                           href="http://www.yzhu.io/projects/tvcg16_gravity/index.html">
                                            <i class="fa fa-expand fa-lg"></i>
                                        </a>
                                    </span>
                        </li>
                    </ul>

                    <div align="justify" id="pub-bibtex-gravity16tvcg" class="panel-collapse collapse">
<pre>
@article{ye2017martian,
    title={The Martian: Examining Human Physical Judgments across Virtual Gravity Fields},
    author={Ye, Tian and Qi, Siyuan and Kubricht, James and Zhu, Yixin and Lu, Hongjing and Zhu, Song-Chun},
    journal={IEEE Transactions on Visualization and Computer Graphics},
    volume={23},
    number={4},
    pages={1399--1408},
    year={2017},
    publisher={IEEE}
}</pre>
                    </div>
                </div>

            </div>
            <div class="col-lg-1"></div>
        </div>
    </div>
</section>

<!-- Contact Section -->
<section id="more" class="more-section">
    <div class="container">
        <div class="row">
            <div class="col-lg-1"></div>
            <div class="col-lg-10">
                <div class="section-title"><h2>More</h2></div>
                <hr>

                <div class="panel">
                    <ul id="more-tab" class="nav nav-tabs nav-justified">
                        <li class="active dropdown">
                            <a href="#" id="more-tab-col-drop" class="dropdown-toggle" data-toggle="dropdown"
                               style="cursor: pointer">Collaborators <b class="caret"></b></a>
                            <ul class="dropdown-menu" role="menu" aria-labelledby="more-tab-col-drop">
                                <li><a href="#more-col-vcla" tabindex="-1" data-toggle="tab">VCLA</a></li>
                                <li><a href="#more-col-cross" tabindex="-1" data-toggle="tab">Cross-lab</a></li>
                            </ul>
                        </li>
                        <!--<li><a href="#more-cv" data-toggle="tab">Curriculum Vitae</a></li>-->
                        <li><a href="#more-awards" data-toggle="tab">Awards</a></li>
                    </ul>
                    <div id="more-tab-content" class="tab-content">
                        <div class="tab-pane fade" id="more-col-vcla">
                            <ul class="list-group">
                                <li class="list-group-item">
                                    <a target="_blank" href="http://www.yzhu.io/index.html">Yixin Zhu</a> and
                                    <a target="_blank" href="https://thusiyuan.github.io/">Siyuan Huang</a> on Scene
                                    Generation and Synthesis.
                                </li>
                                <li class="list-group-item">
                                    <a target="_blank" href="http://www.stat.ucla.edu/~pwei/">Ping Wei</a> on Human
                                    Activity Understanding.
                                </li>
                                <li class="list-group-item">
                                    <a target="_blank" href="http://www.mjedmonds.com/">Mark Edmonds</a> and
                                    <a target="_blank" href="https://fen9.github.io/">Feng Gao</a> on Robot Learning.
                                </li>
                                <li class="list-group-item">
                                    <a target="_blank" href="http://jlin.crevado.com/">Jenny Lin</a>,
                                    <a target="_blank" href="https://xwguo.github.io/">Xingwen Guo</a> and Ye Tian on
                                    Virtual Reality.
                                </li>
                            </ul>
                        </div>
                        <div class="tab-pane fade active in" id="more-col-cross">
                            <ul class="list-group">
                                <li class="list-group-item">
                                    <a target="_blank" href="http://www.seas.upenn.edu/~cffjiang/">Dr. Chenfanfu
                                        Jiang</a> at
                                    <a target="_blank" href="http://cg.cis.upenn.edu/">UPenn Computer Graphics Group</a>
                                </li>
                                <li class="list-group-item">
                                    <a target="_blank" href="http://web.cs.ucla.edu/~dt/">Prof. Demetri Terzopoulos</a>
                                    at
                                    <a target="_blank" href="http://www.magix.ucla.edu/">UCLA Computer Graphics & Vision
                                        Laboratory</a>
                                </li>
                                <li class="list-group-item">
                                    Dr. Sara Spotorno,
                                    <a target="_blank" href="https://sites.google.com/site/skytianxu/">Tian Xu</a> and
                                    <a target="_blank"
                                       href="http://www.gla.ac.uk/researchinstitutes/neurosciencepsychology/staff/philippeschyns/">Prof.
                                        Philippe Schyns</a> at
                                    <a target="_blank" href="http://www.ccni.gla.ac.uk/">University of Glasgow, Centre
                                        for Cognitive Neuroimaging</a>
                                </li>
                                <li class="list-group-item">
                                    <a target="_blank"
                                       href="http://reasoninglab.psych.ucla.edu/Labbies/JamesKubricht.html">James
                                        Kubricht</a> and
                                    <a target="_blank" href="http://cvl.psych.ucla.edu/people.htm">Prof. Hongjing Lu</a>
                                    at
                                    <a target="_blank" href="http://cvl.psych.ucla.edu/">UCLA Computational Vision and
                                        Learning Lab (CVL)</a>
                                </li>
                                <li class="list-group-item">
                                    <a target="_blank" href="http://www.stat.ucla.edu/~ybzhao/">Dr. Yibiao Zhao</a> at
                                    <a target="_blank" href="http://cocosci.mit.edu/">MIT Computational Cognitive
                                        Science Group (cocosci)</a>
                                </li>
                                <li class="list-group-item">
                                    <a target="_blank" href="http://www.cs.umb.edu/~craigyu/">Lap-Fai (Craig) Yu</a> at
                                    University of Massachusetts Boston, Graphics and Virtual Environments Lab
                                </li>
                            </ul>
                        </div>
                        <!--<div class="tab-pane fade" id="more-cv">-->
                        <!--</div>-->
                        <div class="tab-pane fade" id="more-awards">
                            <ul class="list-group">
                                <li class="list-group-item">ICML Travel Award, The International Machine Learning
                                    Society
                                    <div class="pull-right">2018</div>
                                </li>
                                <li class="list-group-item">ICRA Travel Grant, IEEE Robotics and Automation Society
                                    <div class="pull-right">2018</div>
                                </li>
                                <li class="list-group-item">First Class Honors, Faculty of Engineering, University of
                                    Hong Kong
                                    <div class="pull-right">2013</div>
                                </li>
                                <li class="list-group-item">Undergraduate Research Fellowship, University of Hong Kong
                                    <div class="pull-right">2012</div>
                                </li>
                                <li class="list-group-item">Kingboard Scholarship, University of Hong Kong
                                    <div class="pull-right">2010 & 2011 & 2012</div>
                                </li>
                                <li class="list-group-item">Dean's Honors List, University of Hong Kong
                                    <div class="pull-right">2010 & 2011</div>
                                </li>
                                <li class="list-group-item">AI Challenge (Sponsored by Google), 2nd place in Chinese
                                    contestants, 74th worldwide
                                    <div class="pull-right">2011</div>
                                </li>
                                <li class="list-group-item">Student Ambassador, University of Hong Kong
                                    <div class="pull-right">2010</div>
                                </li>
                                <li class="list-group-item">University Entrance Scholarship, University of Hong Kong
                                    <div class="pull-right">2010</div>
                                </li>
                            </ul>
                        </div>
                    </div>
                </div>

            </div>
            <div class="col-lg-1"></div>
        </div>
    </div>
</section>

<div class="footer text-center">
    <div class="footer-copyright text-center">Copyright &copy; 2017 Siyuan Qi</div>
</div>


<!-------------------- JavaScript and Google Analytics -------------------->
<script src="https://code.jquery.com/jquery-3.2.0.min.js"
        integrity="sha256-JAW99MJVpJBGcbzEuXk4Az05s/XyDdBomFqNlM3ic+I=" crossorigin="anonymous"></script>
<script>window.jQuery || document.write('<script src="js/vendor/jquery-3.2.0.min.js"><\/script>')</script>

<!-- Bootstrap Core JavaScript -->
<script src="js/vendor/bootstrap.min.js"></script>

<!-- Bootflat's JS files.-->
<script src="js/vendor/bootflat/icheck.min.js"></script>
<script src="js/vendor/bootflat/jquery.fs.selecter.min.js"></script>
<script src="js/vendor/bootflat/jquery.fs.stepper.min.js"></script>

<!-- Scrolling Nav JavaScript -->
<script src="js/vendor/jquery.easing.min.js"></script>
<script src="js/scrolling-nav.js"></script>

<!-- Custom JS files.-->
<script src="js/plugins.js"></script>
<script src="js/main.js"></script>

<!-- Google Analytics: change UA-XXXXX-Y to be your site's ID. -->
<script>
    if (window.location.host === "www.cs.ucla.edu" || window.location.host === "web.cs.ucla.edu" || window.location.host === "cs.ucla.edu" || window.location.host === "siyuanqi.github.io") {
        window.ga = function () {
            ga.q.push(arguments)
        };
        ga.q = [];
        ga.l = +new Date;
        ga('create', 'UA-83051006-1', 'auto');
        ga('send', 'pageview')
    }
</script>
<script src="https://www.google-analytics.com/analytics.js" async defer></script>
</body>
</html>
